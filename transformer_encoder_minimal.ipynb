{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9a4c5007-649a-4e91-8a97-162c943538bc",
   "metadata": {},
   "source": [
    "# Transformer Encoder for Energy Reconstruction in Telescope Array Experiment\n",
    "\n",
    "This notebook demonstrates how to build a native PyTorch transformer-based neural network for analyzing cosmic ray data from the Telescope Array experiment. \n",
    "The model processes data from activated detectors for each cosmic-ray-induced event and predicts the energy of the primary particle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527e218-5e39-44a9-bc31-38cc257898df",
   "metadata": {},
   "source": [
    "# 1. Import required libraries\n",
    "\n",
    "We will use:\n",
    "\n",
    "- PyTorch: For creating and training neural networks\n",
    "- NumPy: For numerical operations and data handling\n",
    "- h5py: For reading HDF5 files containing our experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20227503-5441-4ec6-b841-de3c5cfc5900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import h5py as h5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e330d78-1ba5-408c-a156-4ebc41acfc70",
   "metadata": {},
   "source": [
    "# 2. Data Generation and Preprocessing\n",
    "\n",
    "In the Telescope Array experiment, cosmic ray events are detected by an array of surface detectors spread across a large area.\n",
    "We will use data that was passed thorough the reconstruction procedure and passes both *composition* and *spectrum* cuts. \n",
    "\n",
    "Full information on an event is given by a set of all triggered detectors.  \n",
    "Each detector is characterized by 5 features:\n",
    "- Its x, y, z coordinates (spatial position)\n",
    "- Integral registered charge (energy deposited)\n",
    "- Time of the plane from arrival (obtained from the reconstruction procedure)\n",
    "- Difference in time between plane front arrival and and actual activation (helps analyze wavefront curvature)\n",
    "\n",
    "The number of activated detectors varies from event to event. \n",
    "To pass them through NN, one should cast them to a unform \"length\" in the following way:\n",
    "- For each batch, we find the maximum number of triggered detectors (`max_event_length`)\n",
    "- Events with fewer detectors are padded with \"auxiliary detectors\" (zeros)\n",
    "- We add a mask channel (value 1 for real detectors, 0 for padding) to allow the network to distinguish real data from padding\n",
    "\n",
    "This results in input tensors with shape (batch_size, max_event_length, 7) where the last dimension includes the 6 detector features plus the mask. The neural network will be designed to ignore these auxiliary detectors.\n",
    "\n",
    "Our neural network will predict the logarithm (base 10) of the primary particle energy. The ground truth values are extracted from the simulation data.\n",
    "\n",
    "## Some technical remarks\n",
    "\n",
    "Datsets has an option to augment data with noise. This allows to avoid overfitting and make NNs prediction more robust.\n",
    "\n",
    "For convenience, the train dataset is made infinite via self-looping.\n",
    "\n",
    "Detectors data is stored in a two dimensional array `dt_params` with shape `(total_detectors, 5)`, where `total_detectors` is the total number of detectors activated in all events (all detectors data is concatenated in a single array).\n",
    "External indexing array `ev_starts` is used to extract data for a required event: for i-th events, the corresponding data is `data_i = dt_params[start:stop]`, where `start=ev_starts[i]` and `start=ev_starts[i+1]`.\n",
    "In particular, event length array can be obtained as `np.diff(ev_starts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3712f740-1fe5-488d-b23a-c90ed8c4450a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default padding values for sequences shorter than the maximum length\n",
    "dense_def_vals = torch.tensor([[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], dtype=torch.float32)\n",
    "\n",
    "class DatasetGenerator(IterableDataset):\n",
    "    \"\"\"\n",
    "    Generates batches of detector data from HDF5 files for training and testing.\n",
    "    \n",
    "    - Handles variable-length sequences via padding\n",
    "    - Supports data augmentation with Gaussian noise (additive and multiplicative)\n",
    "    - Creates an infinite dataset for training via self-loop\n",
    "    - Properly initializes workes so each of them reads its own part of data\n",
    "    \"\"\"\n",
    "    def __init__(self, file, regime, batch_size, return_reminder,\n",
    "                 apply_add_gauss, gauss_stds,\n",
    "                 apply_mult_gauss, mult_gauss_std\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset generator.\n",
    "        \n",
    "        Parameters:\n",
    "        - file: Path to the HDF5 file containing the training data\n",
    "        - regime: 'train' or 'test' mode\n",
    "        - batch_size: Number of events per batch\n",
    "        - return_reminder: Whether to return the last incomplete batch\n",
    "        - apply_add_gauss: Apply additive Gaussian noise for data augmentation\n",
    "        - gauss_stds: Standard deviations for the additive noise in physical units\n",
    "        - apply_mult_gauss: Apply multiplicative Gaussian noise\n",
    "        - mult_gauss_std: Standard deviation for multiplicative noise\n",
    "        \"\"\"\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.apply_add_gauss = apply_add_gauss\n",
    "        self.apply_mult_gauss = apply_mult_gauss\n",
    "        self.g_mult_stds = mult_gauss_std\n",
    "\n",
    "        # Get normalization parameters and dataset size from the HDF5 file\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            mean = hf['norm_param/dt_params/mean'][3]\n",
    "            std = hf['norm_param/dt_params/std'][3]\n",
    "            self.num = hf[self.regime+'/ev_starts'].shape[0]-1\n",
    "        \n",
    "        # Configure noise parameters\n",
    "        if apply_add_gauss:\n",
    "            self.guass_add_stds = gauss_stds / std\n",
    "        if apply_mult_gauss:\n",
    "            self.Q_mean_noise = mean / std\n",
    "            self.n_fraction = mult_gauss_std\n",
    "\n",
    "        # Determine the stop index (end of dataset or last complete batch)\n",
    "        batch_num = self.num // self.batch_size\n",
    "        self.stop = self.num if return_reminder else self.batch_size * batch_num\n",
    "\n",
    "    # Add Gaussian noise to the data for augmentation.\n",
    "    def add_gauss(self, data, std):\n",
    "        noise = np.random.normal(scale=self.guass_add_stds, size=data.shape)\n",
    "        data += noise\n",
    "        return data\n",
    "\n",
    "    # Apply multiplicative Gaussian noise to charge values.\n",
    "    def mult_gauss(self, Qs):\n",
    "        noises = np.random.normal(scale=self.n_fraction, size=Qs.shape)\n",
    "        return Qs + noises * (Qs + self.Q_mean_noise)\n",
    "\n",
    "    # \n",
    "    def step(self, hf, start_ev, stop_ev, start_det, stop_det):\n",
    "        \"\"\"\n",
    "        Process a batch of events from the HDF5 file.\n",
    "        - start_ev, stop_ev: Start and stop indices for events\n",
    "        - start_det, stop_det: Start and stop indices for detectors\n",
    "\n",
    "        Returns:\n",
    "        - dt_params: Detector parameters for events (as 2D array)\n",
    "        - energy_labels: True energy values (log10 scale)\n",
    "        \"\"\"\n",
    "        # Read detector parameters for events\n",
    "        dt_params = hf[self.regime+'/dt_params'][start_det:stop_det]\n",
    "        # Extract energy labels (log10 of primary particle energy)\n",
    "        energy_labels = np.log10(hf[self.regime+'/mc_params/'][start_ev:stop_ev,3:4])\n",
    "        # Apply data augmentation if enabled\n",
    "        if self.apply_add_gauss:\n",
    "            dt_params = self.add_gauss(dt_params, self.guass_add_stds)\n",
    "        if self.apply_mult_gauss:\n",
    "            dt_params[...,3] = self.mult_gauss(dt_params[...,3])\n",
    "        \n",
    "        return dt_params, energy_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        An iterator that yields batches of data.\n",
    "    \n",
    "        For training data, this creates an infinite dataset via self-loop.\n",
    "        For test data, this iterates once through the dataset.\n",
    "        \n",
    "        Yields:\n",
    "        - padded: Padded detector data with mask [batch_size, max_seq_len, 7]\n",
    "        - labels: Energy labels [batch_size, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize multiple workers\n",
    "        # Get worker information\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        # Determine the range of data this worker should process\n",
    "        if worker_info is None:  # single-process data loading\n",
    "            worker_start = 0\n",
    "            worker_end = self.stop\n",
    "        else:  # in a worker process\n",
    "            # Split workload \n",
    "            per_worker = int(np.ceil(self.stop / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            \n",
    "            worker_start = worker_id * per_worker\n",
    "            worker_end = min(worker_start + per_worker, self.stop)\n",
    "            \n",
    "            # Adjust to batch boundaries\n",
    "            worker_start = (worker_start // self.batch_size) * self.batch_size\n",
    "            worker_end = min(((worker_end + self.batch_size - 1) // self.batch_size) * self.batch_size, self.stop)\n",
    "        \n",
    "        # Open the HDF5 file within __iter__ so that each worker gets its own handle.\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            start_ev = worker_start\n",
    "            \n",
    "            iterate = True\n",
    "            while iterate:\n",
    "                stop_ev = start_ev + self.batch_size\n",
    "                \n",
    "                # Check if we've reached the end of this worker's range\n",
    "                if stop_ev > worker_end:\n",
    "                    # For training, make infinite dataset by resetting to start\n",
    "                    if self.regime == 'train':\n",
    "                        # Reset back to the start of this worker's range\n",
    "                        start_ev = worker_start\n",
    "                        stop_ev = start_ev + self.batch_size\n",
    "                    else:\n",
    "                        iterate = False\n",
    "                \n",
    "                # Read detector indices for events\n",
    "                ev_idxs = hf[self.regime+'/ev_starts'][start_ev:stop_ev+1]\n",
    "                # Get detector parameters and energy labels\n",
    "                dt_params, labels = self.step(hf, start_ev, stop_ev, ev_idxs[0], ev_idxs[-1] )\n",
    "\n",
    "                # Make regular tensors\n",
    "                # Calculate the number of detectors per event\n",
    "                raw_lens = np.diff(ev_idxs).astype(np.int64)\n",
    "                max_len = raw_lens.max() # Maximum sequence length in this batch\n",
    "\n",
    "                # Convert the actual data to torch tensors.\n",
    "                data = torch.from_numpy(dt_params)      # shape: (total_dets, 6)\n",
    "                labels = torch.from_numpy(labels)       # shape: (total_evs, 1)\n",
    "                \n",
    "                # Create mask: 1 for real detectors, 0 for padding\n",
    "                mask = torch.ones((data.shape[0], 1), dtype=torch.float32) # shape: (total_dets, 1)\n",
    "                # Concatenate detector features and mask\n",
    "                data = torch.cat([data, mask], dim=-1)\n",
    "                \n",
    "                # Preallocate padded tensor: shape (batch_size, max_len, 7), filled with default values.\n",
    "                padded = torch.tile(dense_def_vals, (labels.shape[0], max_len, 1))\n",
    "                \n",
    "                # Create a boolean mask with shape (batch_size, max_len).\n",
    "                # For each event, positions [0, raw_lens[i]) are True.\n",
    "                mask = np.arange(max_len)[None, :] < raw_lens[:, None]  # shape: (batch_size, max_len)\n",
    "                mask_tensor = torch.from_numpy(mask)\n",
    "                \n",
    "                # Fill the padded tensor with actual data\n",
    "                padded[mask_tensor] = data\n",
    "\n",
    "                # Move to next batch\n",
    "                start_ev += self.batch_size\n",
    "\n",
    "                yield padded.float(), labels.float() # also convert to float32\n",
    "\n",
    "def make_datasets(file, batch_size,\n",
    "                 apply_add_gauss, gauss_stds,\n",
    "                 apply_mult_gauss, mult_gauss_std):\n",
    "    \"\"\"\n",
    "    Create train and test datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - file: Path to the HDF5 file\n",
    "    - batch_size: Number of events per batch\n",
    "    - apply_add_gauss: Whether to apply additive Gaussian noise (for augmentation)\n",
    "    - gauss_stds: Standard deviations for additive noise\n",
    "    - apply_mult_gauss: Whether to apply multiplicative Gaussian noise\n",
    "    - mult_gauss_std: Standard deviation for multiplicative noise\n",
    "    \n",
    "    Returns:\n",
    "    - train_dataset: DataLoader for training\n",
    "    - test_dataset: DataLoader for testing\n",
    "    \"\"\"\n",
    "    # Create generators for train and test datasets\n",
    "    train_generator = DatasetGenerator(file, 'train', batch_size, False,\n",
    "                 apply_add_gauss, gauss_stds,\n",
    "                 apply_mult_gauss, mult_gauss_std)\n",
    "    test_generator = DatasetGenerator(file, 'test', batch_size, False,\n",
    "                 False, None,\n",
    "                 False, None)\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = DataLoader(train_generator, batch_size=None, shuffle=False, pin_memory=True, num_workers=0, prefetch_factor=None)\n",
    "    test_dataset = DataLoader(test_generator, batch_size=None, shuffle=False, pin_memory=True, num_workers=0, prefetch_factor=None)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab855b83-dd55-4b9b-9287-46cfa68d61ee",
   "metadata": {},
   "source": [
    "## Take a look at data\n",
    "\n",
    "Below we initiate dataset generator and take a look at one batch.\n",
    "Each event is padded to the maximal \"length\" in the batch and auxiliary detectors are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a99ced9-ef11-4163-b8ac-34308a30cf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "  'file' : '/home3/ivkhar/TA/data/normed/composition_spectrum/taml_0325_energy.h5', # path to training file\n",
    "  'batch_size' : 4, # batch size\n",
    "  'apply_add_gauss' : False, # flag for addative data augmentation\n",
    "  'gauss_stds' : [0., 0., 0., 0., 0. , 0.], # noise parameters\n",
    "  'apply_mult_gauss' : False, # flag for multiplicative augmentation of registered charges\n",
    "  'mult_gauss_std' : 0.0 # noise parameters\n",
    "}\n",
    "\n",
    "train_generator = DatasetGenerator(regime='train', return_reminder=False, **generator_config)\n",
    "\n",
    "for data, label in train_generator.__iter__():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5107713-0b96-4049-9415-1d69f0af469e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 34, 7]) torch.Size([4, 1])\n",
      "tensor([[[ 0.5414, -1.5941, -1.2051, -0.4940, -0.5042, -0.3481,  1.0000],\n",
      "         [ 0.5461, -0.8243, -1.1074, -0.4972, -0.2328, -0.7657,  1.0000],\n",
      "         [-0.1986, -0.7951, -1.0037, -0.4665, -0.0465, -0.6553,  1.0000],\n",
      "         [ 0.5435, -0.0488, -1.0852,  0.0252,  0.0443, -0.8896,  1.0000],\n",
      "         [ 0.5435, -0.0488, -1.0852,  0.0252,  0.0443,  0.3297,  1.0000],\n",
      "         [ 0.5442,  0.7246, -0.9282, -0.4936,  0.3160, -0.7222,  1.0000],\n",
      "         [-0.9999, -0.0454, -1.0370, -0.4302,  0.4144, -0.9027,  1.0000],\n",
      "         [-0.2268,  0.7237, -0.8708, -0.3854,  0.4993, -0.7661,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.7073, -0.1596, -0.5008, -0.4781, -1.1907, -0.6759,  1.0000],\n",
      "         [-1.7073, -0.1596, -0.5008, -0.4781, -1.1907,  0.3376,  1.0000],\n",
      "         [-0.9377, -1.0043, -0.6628, -0.4801, -0.7227, -0.4807,  1.0000],\n",
      "         [-0.9377, -1.0043, -0.6628, -0.4801, -0.7227,  0.5328,  1.0000],\n",
      "         [-0.9682, -0.1480, -0.4024, -0.2129, -0.6126, -0.8489,  1.0000],\n",
      "         [-0.9682, -0.1480, -0.4024, -0.2129, -0.6126,  0.4259,  1.0000],\n",
      "         [-0.9682, -0.1480, -0.4024, -0.2129, -0.6126,  1.6294,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606, -0.7372,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606,  0.4346,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606,  1.4480,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606,  2.4615,  1.0000],\n",
      "         [-0.1643, -2.4843, -0.6656, -0.4904, -0.3595,  0.5280,  1.0000],\n",
      "         [-0.9321,  2.1536, -0.4799, -0.4921, -0.2065, -0.6672,  1.0000],\n",
      "         [-0.1642, -0.9376, -0.5772, -0.2701, -0.1087, -0.8631,  1.0000],\n",
      "         [-0.1642, -0.9376, -0.5772, -0.2701, -0.1087,  0.4670,  1.0000],\n",
      "         [-0.1962,  0.6206, -0.5225,  0.6251,  0.1194, -0.8892,  1.0000],\n",
      "         [-0.1962,  0.6206, -0.5225,  0.6251,  0.1194,  0.1242,  1.0000],\n",
      "         [-0.1612,  1.3805, -0.5596, -0.4496,  0.2721, -0.4134,  1.0000],\n",
      "         [-0.1612,  1.3805, -0.5596, -0.4496,  0.2721,  0.6001,  1.0000],\n",
      "         [ 0.6062, -1.7113, -0.7420, -0.4982,  0.3715, -0.0330,  1.0000],\n",
      "         [ 0.6078, -0.9393, -0.6400, -0.3373,  0.4966, -0.7123,  1.0000],\n",
      "         [ 0.6078, -0.9393, -0.6400, -0.3373,  0.4966,  0.3803,  1.0000],\n",
      "         [ 0.6094, -0.1638, -0.6697,  1.5633,  0.6250, -0.8972,  1.0000],\n",
      "         [ 0.6094, -0.1638, -0.6697,  1.5633,  0.6250,  0.1163,  1.0000],\n",
      "         [ 0.6416,  0.6082, -0.5930, -0.0728,  0.7747, -0.8378,  1.0000],\n",
      "         [ 0.6416,  0.6082, -0.5930, -0.0728,  0.7747,  0.1994,  1.0000],\n",
      "         [ 0.6416,  0.6082, -0.5930, -0.0728,  0.7747,  2.1313,  1.0000],\n",
      "         [ 1.3792, -0.9402, -0.5980, -0.4531,  1.0989, -0.3920,  1.0000],\n",
      "         [ 1.3792, -0.9402, -0.5980, -0.4531,  1.0989,  0.6927,  1.0000],\n",
      "         [ 1.3808, -0.1990, -0.7767, -0.4667,  1.2253, -0.6616,  1.0000],\n",
      "         [ 1.3845,  0.6058, -0.7530, -0.4715,  1.3590, -0.6466,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, label.shape)\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb7e406-5817-4af2-8258-a492a10f08af",
   "metadata": {},
   "source": [
    "# 3. Metrics\n",
    "\n",
    "When training neural networks, we need metrics to track performance. \n",
    "Here we implement a simple metric that tracks the average value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "181bc0f8-34b8-4dd4-b112-1796e3f930b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanMetrics(nn.Module):\n",
    "\n",
    "    # Initialize metric\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.register_buffer('value', torch.tensor(0.0))\n",
    "        self.register_buffer('steps', torch.tensor(0.0))\n",
    "        self.name = name\n",
    "\n",
    "    # Define how to reset metric\n",
    "    def reset(self):\n",
    "        self.value.zero_()\n",
    "        self.steps.zero_()\n",
    "\n",
    "    # Define how to update state\n",
    "    def update_state(self, value):\n",
    "        self.value.add_(value.detach())\n",
    "        self.steps.add_(1.)\n",
    "\n",
    "    # Define the resulting metric value\n",
    "    def result(self):\n",
    "        return self.value / (self.steps + 1e-9) # Add small epsilon to avoid division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc5d1d5-26c4-4976-9dfd-36d0f46bed48",
   "metadata": {},
   "source": [
    "# 4. Neural netowrk architecture\n",
    "\n",
    "For minumal implementation, we will need to define: \n",
    "- Embedding layer (simple linear layer). It is needed to increase dimensionality of the data to fit transformer dimsnionality.\n",
    "- Aggregation layer. How we gather data from all detectors to obtain a signle feature vector.\n",
    "- Prediction layer (MLP). Combination of linear layers that predicts the target value - log10(E).\n",
    "\n",
    "Further we will combine them with PyTorch implementation of the transformer architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04837fad-9579-46c7-892c-d360fb3c0106",
   "metadata": {},
   "source": [
    "## 4.1 Embedding layer\n",
    "\n",
    "Our initial data has 6 features, while transformer has higher dimensionality.\n",
    "We need to embed your data to this higher dimensional space.\n",
    "\n",
    "This can be done via a single linear layer.\n",
    "To preserve physical data, we initialize it as identity matrix and allow NN to optimize it.\n",
    "\n",
    "### On positional encoding\n",
    "\n",
    "Position encoding is needed when node encodings do not provide positional information (for example, tokens in natural language processing).\n",
    "In our case, nodes (detectors) has both temporal and spatial information, which yield positional encoding redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fb82419-1047-4914-a763-f585daf967b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        \"\"\"\n",
    "        - dim_in: Input dimension (number of detector features)\n",
    "        - dim_out: Hidden layer dimension (transformer dimensionality)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        # Initialize matrix as identity\n",
    "        nn.init.eye_(self.dense.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linearly map to transformer dimensionality \n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d12c46-99d0-494f-b873-6eccfbacb5dd",
   "metadata": {},
   "source": [
    "## 4.2 Aggregation Layer\n",
    "\n",
    "To infer energy of the primary particle, we need to aggregate data from all detectors. \n",
    "We will use averaging for this purpose. \n",
    "\n",
    "Another approach is to introduce classifiaction token.\n",
    "We will not use this advanced technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11e57897-5d98-4eb3-a50f-1284304d69e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\" \n",
    "        Return average taking into account mask\n",
    "        Parameters:\n",
    "        - x: data tensor [batch_size, seq_len, d_model]\n",
    "        - mask: mask for auxiliary detectors [batch_size, 1]\n",
    "        \"\"\"\n",
    "        return torch.sum(x*mask, dim=1) / torch.sum(mask, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787601b-357f-481b-af8c-342eb6a59ce0",
   "metadata": {},
   "source": [
    "## 4.3 Prediction Layer\n",
    "\n",
    "The Prediction Layer is the final component of our network that takes the aggregated detector information and transforms it into the energy prediction.\n",
    "After the transformer encoder processes and contextualizes all detector activations, we need a way to combine this information and map it to our target variable: the log10 of the primary particle's energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0901e51-e0e8-4c30-8d4c-8ee810ffc927",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Final prediction layer that processes aggregated detector data.\n",
    "    \n",
    "    This layer implements a multi-layer perceptron (MLP) that maps from\n",
    "    the transformer's high-dimensional representation space to the \n",
    "    energy prediction (log10 scale). It consists of:\n",
    "    \n",
    "    1. A reduction layer that maps from the model dimension to a smaller dimension\n",
    "    2. Optional intermediate layers for additional capacity\n",
    "    3. A final output layer that produces the energy prediction\n",
    "    \n",
    "    Each layer is followed by a Leaky ReLU activation, except the final output\n",
    "    which produces a raw scalar value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_middle, dim_out, num_middle_layers):\n",
    "        \"\"\"\n",
    "        - dim_in: Input dimension (from transformer encoder, typically d_model)\n",
    "        - dim_middle: Hidden layer dimension (smaller than dim_in for efficiency)\n",
    "        - dim_out: Output dimension (1 for energy prediction)\n",
    "        - num_middle_layers: Number of hidden layers between reduction and output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initial dimension reduction\n",
    "        # This compresses the high-dimensional detector representation\n",
    "        # to a more manageable size for the regression task\n",
    "        self.reduce = nn.Linear(dim_in, dim_middle)\n",
    "        # Hidden layers for additional modeling capacity\n",
    "        # Each layer maintains the same dimension (dim_middle)\n",
    "        self.pre_layers = nn.ModuleList([\n",
    "            nn.Linear(dim_middle, dim_middle) for _ in range(num_middle_layers)\n",
    "        ])\n",
    "        # Final output layer that produces the energy prediction\n",
    "        # Maps from the hidden dimension to a single scalar output\n",
    "        self.out = nn.Linear(dim_middle, dim_out)\n",
    "        # Leaky ReLU activation function\n",
    "        self.activation = F.leaky_relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - x: Input tensor [batch_size, dim_in]\n",
    "          This contains aggregated information from all detectors for each event\n",
    "        \n",
    "        Returns:\n",
    "        - Predictions [batch_size, dim_out]\n",
    "          Log10 of the predicted energy for each event\n",
    "        \"\"\"\n",
    "        # Initial dimension reduction with activation\n",
    "        x = self.reduce(x)\n",
    "        x = self.activation(x)\n",
    "        # Apply each hidden layer with activation\n",
    "        for layer in self.pre_layers:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        # Final prediction layer (no activation)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b76e456-c323-41c3-87d0-0afe171f90a6",
   "metadata": {},
   "source": [
    "## 4.4 Complete Encoder Model\n",
    "\n",
    "To define an encoder we need to combine layers:\n",
    "1. Embedding layer to increase dimensionality of the data.\n",
    "2. PyTorch Transformer-Encoder Layer. It will process the data and extracting features of interest.\n",
    "3. Aggregation layer to aggregate information from all detectors to a single feature vector.\n",
    "4. Prediction layer to predict the log10 of the energy.\n",
    "\n",
    "We will use mask to prohibit \"real\" detectors to paying attention to auxiliary ones. \n",
    "The mask is created as follows:\n",
    "- Start with a 1D mask of shape [batch_size, seq_len, 1], in which 1 indicates a real detector and 0 indicates padding\n",
    "- Create a 2D mask by multiplying this vector with its transpose: mask * mask.transpose(1,2)\n",
    "- This gives a matrix of shape [batch_size, seq_len, seq_len] where position (i,j) is 1 only if both detector i and j are real\n",
    "- Expand this to include the heads dimension: [batch_size, 1, seq_len, seq_len]. This dimensional expasion is required for proper broadcasting with multi head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9952145-45d0-4a3b-99f0-adf39dd8262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, head_dim, input_dim, dropout,\n",
    "                    dim_middle_pred, dim_out_pred, num_middle_layers_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_layers: Number of stacked encoder layers (depth of the model)\n",
    "        - num_heads: Number of attention heads in each encoder layer\n",
    "          (allows the model to focus on different aspects of the data)\n",
    "        - d_model: Model dimension - internal representation size for detector features\n",
    "        - d_ff: Feed-forward hidden dimension (typically 4x d_model)\n",
    "        - head_dim: Dimension of each attention head (d_model / num_heads)\n",
    "        - input_dim: Input feature dimension (6 detector features + 1 mask)\n",
    "        - dropout: Dropout rate for regularization\n",
    "        - dim_middle_pred: Hidden dimension in the prediction layer\n",
    "        - dim_out_pred: Output dimension (1 for energy prediction)\n",
    "        - num_middle_layers_pred: Number of hidden layers in prediction network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # Embedding layer: Projects the 6 detector features into the model dimension\n",
    "        # We use input_dim-1 because the last dimension is the mask\n",
    "        self.embedding_layer = EmbeddingLayer(input_dim-1, d_model)\n",
    "\n",
    "        # This is how to initialize PyToch Transformer layer\n",
    "        # PyTorch native transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            #norm_first=True\n",
    "        )       \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "               \n",
    "        # Aggregation layer: aggragate data from all detectors.\n",
    "        # It will be used to infer log10 energyof the primary particle.\n",
    "        self.aggr_layer = AggregateLayer()\n",
    "        # Prediction layer: Takes the aggregated detector information and\n",
    "        # predicts the energy of the primary particle\n",
    "        self.predict_layer = PredictLayer(d_model, dim_middle_pred, dim_out_pred, num_middle_layers_pred)\n",
    "\n",
    "    def compile(self, optim_kwargs, scheduler_kwargs):\n",
    "        \"\"\"\n",
    "        Configure the model for training by defining loss function, optimizer,\n",
    "        learning rate scheduler, and evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        - optim_kwargs: Optimizer parameters\n",
    "        - scheduler_kwargs: Learning rate scheduler parameters\n",
    "        \"\"\"\n",
    "        # Define loss function - Mean Squared Error for regression task\n",
    "        self.loss = nn.functional.mse_loss\n",
    "        # Configure Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), **optim_kwargs)\n",
    "        # Learning rate scheduler that reduces LR when performance plateaus\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **scheduler_kwargs)\n",
    "        # Define metrics to track during training\n",
    "        metric_names = ['mse_logE_loss']\n",
    "        self.metrics = [ MeanMetrics(name=mn) for mn in metric_names ]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"  \n",
    "        The data flow follows these steps:\n",
    "        1. Split data into detector features and mask\n",
    "        2. Create attention mask to prevent attending to padding\n",
    "        3. Embed detector features into higher-dimensional space\n",
    "        4. Process through multiple encoder layers\n",
    "        5. Aggregate information from all detectors\n",
    "        6. Generate energy prediction\n",
    "        \n",
    "        Parameters:\n",
    "        - inputs: Input tensor with shape [batch_size, seq_len, input_dim]\n",
    "          where seq_len is the maximum number of detectors in the batch\n",
    "          and the last feature is the mask\n",
    "        \n",
    "        Returns:\n",
    "        - Energy predictions [batch_size, 1] in log10 scale\n",
    "        \"\"\"\n",
    "        # Split input into data features and mask\n",
    "        # x: [batch_size, seq_len, 6] - detector features\n",
    "        # mask: [batch_size, seq_len, 1] - binary mask\n",
    "        x, mask = inputs[:, :, :-1], inputs[:, :, -1:]\n",
    "        # Create 2D attention mask from the 1D feature mask\n",
    "        \n",
    "        # We need to cast mask to the form required by PyTorch\n",
    "        mask_bool = (mask[:,:,0] == 0) \n",
    "        mask_att = torch.tile(mask_bool.unsqueeze(-1) * torch.transpose(mask_bool.unsqueeze(-1), 1, 2), [self.num_heads,1,1]) # (batch_size*num_heads, l, l)\n",
    "        \n",
    "        # Embed input features into the model's higher-dimensional space\n",
    "        # This projection preserves physical meaning while allowing more expressive representations\n",
    "        x = self.embedding_layer(x)\n",
    "        # Process through the stack of encoder layers\n",
    "        # Each layer refines the detector representations\n",
    "        # Mask parameters prevent transformer for attending auxiliary detectors\n",
    "        x = self.transformer_encoder(src=x,\n",
    "            src_key_padding_mask=mask_bool,\n",
    "            mask = mask_att\n",
    "                                    )\n",
    "        # Aggregate data to a single vector\n",
    "        aggr = self.aggr_layer(x, mask) #torch.sum(x*mask, dim=1) / torch.sum(mask, dim=1)\n",
    "        # Generate energy prediction from the aggregated event representation\n",
    "        preds = self.predict_layer(aggr)\n",
    "        return preds\n",
    "\n",
    "    def update_metrics(self, metric_updates):\n",
    "        \"\"\"\n",
    "        Update tracking metrics with new values.\n",
    "        \n",
    "        Parameters:\n",
    "        - metric_updates: List of new metric values from the most recent step\n",
    "        \"\"\"\n",
    "        for m_update, m_tracker in zip(metric_updates, self.metrics):\n",
    "            m_tracker.update_state(m_update)\n",
    "\n",
    "    def train_step(self, data, labels):\n",
    "        \"\"\"\n",
    "        Perform a single training step with backpropagation.\n",
    "        \n",
    "        This method:\n",
    "        1. Sets the model to training mode\n",
    "        2. Performs forward pass\n",
    "        3. Calculates loss\n",
    "        4. Computes gradients via backpropagation\n",
    "        5. Updates model parameters\n",
    "        6. Updates metrics\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input detector data [batch_size, seq_len, input_dim+1]\n",
    "        - labels: True energy values [batch_size, 1] in log10 scale\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of metrics including loss and learning rate\n",
    "        \"\"\"\n",
    "        # Set model to training mode (enables dropout, batch norm updates, etc.)\n",
    "        self.train()\n",
    "        # Zero gradients from previous step\n",
    "        # This is necessary because PyTorch accumulates gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forward pass: Generate predictions\n",
    "        preds = self.forward(data)\n",
    "        # Calculate loss between predictions and true values\n",
    "        loss = self.loss(preds, labels)\n",
    "        # Backward pass: Compute gradient of loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Update weights using the optimizer\n",
    "        self.optimizer.step()\n",
    "        # Update tracking metrics\n",
    "        self.update_metrics([loss])\n",
    "        # Return metrics dictionary for loggin\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}\n",
    "\n",
    "    def test_step(self, data, labels):\n",
    "        \"\"\"\n",
    "        Perform a single validation/test step without parameter updates.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode (disables dropout, freezes batch norm, etc.)\n",
    "        self.eval()\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass: Generate predictions\n",
    "            preds = self.forward(data)\n",
    "            # Calculate loss between predictions and true values\n",
    "            loss = self.loss(preds, labels)\n",
    "        # Update tracking metrics\n",
    "        self.update_metrics([loss])\n",
    "        # Return metrics dictionary for logging\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d371f1-1848-412e-b753-46897aa1c096",
   "metadata": {},
   "source": [
    "# 5. Preparing for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f4982f2-b28f-42c6-aae7-0afd77650db5",
   "metadata": {},
   "source": [
    "## 5.1 Set various configurations\n",
    "\n",
    "### 5.1.1 Set configuration of datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "75d1858c-f91b-4a7b-a91e-2ce2f5947766",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "  'file' : '/home3/ivkhar/TA/data/normed/composition_spectrum/taml_0325_energy.h5', # path to training file\n",
    "  'batch_size' : 128, # batch size\n",
    "  'apply_add_gauss' : False, # flag for addative data augmentation\n",
    "  'gauss_stds' : [0., 0., 0., 0., 0. , 0.], # noise parameters\n",
    "  'apply_mult_gauss' : False, # flag for multiplicative augmentation of registered charges\n",
    "  'mult_gauss_std' : 0.0 # noise parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e64c12-53dd-4b33-be7e-22fc0538438b",
   "metadata": {},
   "source": [
    "### 5.1.2 Set model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1086fdfb-1138-4021-8887-5bb56f6f2b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture parameters\n",
    "nn_arch_params = {\n",
    "  'num_layers': 5,       # Number of transformer encoder layers\n",
    "  'num_heads': 4,        # Number of attention heads per layer\n",
    "  'd_model': 128,        # Model dimension\n",
    "  'd_ff': 512,           # Feed-forward hidden dimension (4 * d_model)\n",
    "  'head_dim': 32,        # Dimension of each attention head\n",
    "  'input_dim': 7,        # Input features dimension (6 + 1 mask)\n",
    "  'dropout': 0.,         # Dropout rate\n",
    "  'dim_middle_pred': 32, # Prediction hidden dimension\n",
    "  'dim_out_pred': 1,     # Output dimension (1 for energy)\n",
    "  'num_middle_layers_pred': 1  # Number of prediction hidden layers\n",
    "}\n",
    "\n",
    "optimizer_params ={\n",
    "  'lr': 0.0005  # Learning rate\n",
    "  }\n",
    "\n",
    "scheduler_params = {\n",
    "  'factor': 0.25,    # Factor to reduce learning rate by\n",
    "  'patience': 4      # Number of epochs with no improvement before reducing LR\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f90a12d-9967-4cb3-b327-f4bfc6a3b5ae",
   "metadata": {},
   "source": [
    "### 5.1.3 Set Training Configuration (including Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b296757-e140-4681-b7a4-f9ccfd42a3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "model_name = 'taml_test'            # Name for saving the model\n",
    "patience = 8                        # Early stopping patience\n",
    "train_steps_per_epoch = 1000         # Number of batches per epoch\n",
    "test_steps_per_epoch = 500          # Number of test batches per epoch\n",
    "min_delta = 1.e-4                   # Minimum improvement for early stopping\n",
    "num_epochs = 10                     # Maximum number of epochs; increase for real training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d34936a-ab17-4e09-be7f-201eb11da1d3",
   "metadata": {},
   "source": [
    "## 5.2 Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "217e48b5-8517-4a16-885c-d9e4c3f0fbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_arch_builder):\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset, test_dataset = make_datasets(**generator_config)\n",
    "\n",
    "    # Create model and move to GPU\n",
    "    model = nn_arch_builder(**nn_arch_params)\n",
    "    model.to('cuda')\n",
    "    # Compile the model\n",
    "    model.compile(optimizer_params, scheduler_params)\n",
    "\n",
    "    # Create infinite training data iterator\n",
    "    train_iter = iter(train_dataset)\n",
    "    \n",
    "    # Initialize early stopping variables\n",
    "    best_loss = 1.e9\n",
    "    wait = 0  # Counter for patience\n",
    "    \n",
    "    # Move metrics to GPU\n",
    "    for metric in model.metrics:\n",
    "        metric.to('cuda')\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "        ### TRAINING PHASE\n",
    "        # Reset metrics\n",
    "        for metric in model.metrics:\n",
    "            metric.reset()\n",
    "    \n",
    "        # Train for specified number of steps\n",
    "        for step in range(train_steps_per_epoch):\n",
    "            # Get next batch from infinite iterator\n",
    "            batch = next(train_iter)\n",
    "            # Move data to GPU\n",
    "            data, labels = [b.to('cuda') for b in batch]\n",
    "            # Perform training step\n",
    "            all_metrics = model.train_step(data, labels)\n",
    "        # Print training metrics\n",
    "        print(f\"Train loss: {all_metrics['mse_logE_loss'].item()}\")\n",
    "    \n",
    "        ### VALIDATION PHASE\n",
    "        # Reset metrics\n",
    "        for metric in model.metrics:\n",
    "            metric.reset()\n",
    "    \n",
    "        # Validate on test dataset\n",
    "        for i, batch in enumerate(test_dataset):\n",
    "            if i >= test_steps_per_epoch:\n",
    "                break\n",
    "            data, labels = [b.to('cuda') for b in batch]\n",
    "            all_metrics = model.test_step(data, labels)\n",
    "            \n",
    "        print(f\"Test loss: {all_metrics['mse_logE_loss'].item()}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        val_loss = all_metrics['mse_logE_loss']\n",
    "        # EarlyStopping and ModelCheckpoint\n",
    "        if val_loss < best_loss - min_delta:\n",
    "            # We have improvement\n",
    "            best_loss = val_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            # No improvement\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "        # Update learning rate based on validation loss\n",
    "        model.scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ac865a-7a20-4a22-9b4f-6b07f1aa48b1",
   "metadata": {},
   "source": [
    "## 5.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ac9d2239-3086-43fe-bb18-d12f7d1193f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 0.07115709036588669\n",
      "Test loss: 0.008752821944653988\n",
      "Epoch 2\n",
      "Train loss: 0.00928259827196598\n",
      "Test loss: 0.009067647159099579\n",
      "Epoch 3\n",
      "Train loss: 0.008210983127355576\n",
      "Test loss: 0.007554568350315094\n",
      "Epoch 4\n",
      "Train loss: 0.007794237229973078\n",
      "Test loss: 0.007981672883033752\n",
      "Epoch 5\n",
      "Train loss: 0.007148415315896273\n",
      "Test loss: 0.007173273712396622\n",
      "Epoch 6\n",
      "Train loss: 0.006880099885165691\n",
      "Test loss: 0.006401329301297665\n",
      "Epoch 7\n",
      "Train loss: 0.006728038191795349\n",
      "Test loss: 0.006596859078854322\n",
      "Epoch 8\n",
      "Train loss: 0.006346757989376783\n",
      "Test loss: 0.006017283536493778\n",
      "Epoch 9\n",
      "Train loss: 0.006508741527795792\n",
      "Test loss: 0.007102299015969038\n",
      "Epoch 10\n",
      "Train loss: 0.006196258589625359\n",
      "Test loss: 0.006346041336655617\n"
     ]
    }
   ],
   "source": [
    "train_model(Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfc1584-9d46-49b8-b2d5-bd52a731296e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
