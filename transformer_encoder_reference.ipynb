{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Encoder for Energy Reconstruction in Telescope Array Experiment\n",
    "\n",
    "This notebook demonstrates how to build a transformer-based neural network for analyzing cosmic ray data from the Telescope Array experiment. \n",
    "The model processes data from activated detectors for each cosmic-ray-induced event and predicts the energy of the primary particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import required libraries\n",
    "\n",
    "We will use:\n",
    "\n",
    "- PyTorch: For creating and training neural networks\n",
    "- NumPy: For numerical operations and data handling\n",
    "- h5py: For reading HDF5 files containing our experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import IterableDataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import h5py as h5\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Generation and Preprocessing\n",
    "\n",
    "In the Telescope Array experiment, cosmic ray events are detected by an array of surface detectors spread across a large area.\n",
    "We will use data that was passed thorough the reconstruction procedure and passes both *composition* and *spectrum* cuts. \n",
    "\n",
    "Full information on an event is given by a set of all triggered detectors.  \n",
    "Each detector is characterized by 5 features:\n",
    "- Its x, y, z coordinates (spatial position)\n",
    "- Integral registered charge (energy deposited)\n",
    "- Time of the plane from arrival (obtained from the reconstruction procedure)\n",
    "- Difference in time between plane front arrival and and actual activation (helps analyze wavefront curvature)\n",
    "\n",
    "The number of activated detectors varies from event to event. \n",
    "To pass them through NN, one should cast them to a unform \"length\" in the following way:\n",
    "- For each batch, we find the maximum number of triggered detectors (`max_event_length`)\n",
    "- Events with fewer detectors are padded with \"auxiliary detectors\" (zeros)\n",
    "- We add a mask channel (value 1 for real detectors, 0 for padding) to allow the network to distinguish real data from padding\n",
    "\n",
    "This results in input tensors with shape (batch_size, max_event_length, 7) where the last dimension includes the 6 detector features plus the mask. The neural network will be designed to ignore these auxiliary detectors.\n",
    "\n",
    "Our neural network will predict the logarithm (base 10) of the primary particle energy. The ground truth values are extracted from the simulation data.\n",
    "\n",
    "## Some technical remarks\n",
    "\n",
    "Datsets has an option to augment data with noise. This allows to avoid overfitting and make NNs prediction more robust.\n",
    "\n",
    "For convenience, the train dataset is made infinite via self-looping.\n",
    "\n",
    "Detectors data is stored in a two dimensional array `dt_params` with shape `(total_detectors, 5)`, where `total_detectors` is the total number of detectors activated in all events (all detectors data is concatenated in a single array).\n",
    "External indexing array `ev_starts` is used to extract data for a required event: for i-th events, the corresponding data is `data_i = dt_params[start:stop]`, where `start=ev_starts[i]` and `start=ev_starts[i+1]`.\n",
    "In particular, event length array can be obtained as `np.diff(ev_starts)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default padding values for sequences shorter than the maximum length\n",
    "dense_def_vals = torch.tensor([[[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]]], dtype=torch.float32)\n",
    "\n",
    "class DatasetGenerator(IterableDataset):\n",
    "    \"\"\"\n",
    "    Generates batches of detector data from HDF5 files for training and testing.\n",
    "    \n",
    "    - Handles variable-length sequences via padding\n",
    "    - Supports data augmentation with Gaussian noise (additive and multiplicative)\n",
    "    - Creates an infinite dataset for training via self-loop\n",
    "    - Properly initializes workes so each of them reads its own part of data\n",
    "    \"\"\"\n",
    "    def __init__(self, file, regime, batch_size, return_reminder,\n",
    "                 apply_add_gauss, gauss_stds,\n",
    "                 apply_mult_gauss, mult_gauss_std\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Initialize the dataset generator.\n",
    "        \n",
    "        Parameters:\n",
    "        - file: Path to the HDF5 file containing the training data\n",
    "        - regime: 'train' or 'test' mode\n",
    "        - batch_size: Number of events per batch\n",
    "        - return_reminder: Whether to return the last incomplete batch\n",
    "        - apply_add_gauss: Apply additive Gaussian noise for data augmentation\n",
    "        - gauss_stds: Standard deviations for the additive noise in physical units\n",
    "        - apply_mult_gauss: Apply multiplicative Gaussian noise\n",
    "        - mult_gauss_std: Standard deviation for multiplicative noise\n",
    "        \"\"\"\n",
    "        self.file = file\n",
    "        self.regime = regime\n",
    "        self.batch_size = batch_size\n",
    "        self.apply_add_gauss = apply_add_gauss\n",
    "        self.apply_mult_gauss = apply_mult_gauss\n",
    "        self.g_mult_stds = mult_gauss_std\n",
    "\n",
    "        # Get normalization parameters and dataset size from the HDF5 file\n",
    "        with h5.File(self.file,'r') as hf:\n",
    "            mean = hf['norm_param/dt_params/mean'][3]\n",
    "            std = hf['norm_param/dt_params/std'][3]\n",
    "            self.num = hf[self.regime+'/ev_starts'].shape[0]-1\n",
    "        \n",
    "        # Configure noise parameters\n",
    "        if apply_add_gauss:\n",
    "            self.guass_add_stds = gauss_stds / std\n",
    "        if apply_mult_gauss:\n",
    "            self.Q_mean_noise = mean / std\n",
    "            self.n_fraction = mult_gauss_std\n",
    "\n",
    "        # Determine the stop index (end of dataset or last complete batch)\n",
    "        batch_num = self.num // self.batch_size\n",
    "        self.stop = self.num if return_reminder else self.batch_size * batch_num\n",
    "\n",
    "    # Add Gaussian noise to the data for augmentation.\n",
    "    def add_gauss(self, data, std):\n",
    "        noise = np.random.normal(scale=self.guass_add_stds, size=data.shape)\n",
    "        data += noise\n",
    "        return data\n",
    "\n",
    "    # Apply multiplicative Gaussian noise to charge values.\n",
    "    def mult_gauss(self, Qs):\n",
    "        noises = np.random.normal(scale=self.n_fraction, size=Qs.shape)\n",
    "        return Qs + noises * (Qs + self.Q_mean_noise)\n",
    "\n",
    "    # \n",
    "    def step(self, hf, start_ev, stop_ev, start_det, stop_det):\n",
    "        \"\"\"\n",
    "        Process a batch of events from the HDF5 file.\n",
    "        - start_ev, stop_ev: Start and stop indices for events\n",
    "        - start_det, stop_det: Start and stop indices for detectors\n",
    "\n",
    "        Returns:\n",
    "        - dt_params: Detector parameters for events (as 2D array)\n",
    "        - energy_labels: True energy values (log10 scale)\n",
    "        \"\"\"\n",
    "        # Read detector parameters for events\n",
    "        dt_params = hf[self.regime+'/dt_params'][start_det:stop_det]\n",
    "        # Extract energy labels (log10 of primary particle energy)\n",
    "        energy_labels = np.log10(hf[self.regime+'/mc_params/'][start_ev:stop_ev,3:4])\n",
    "        # Apply data augmentation if enabled\n",
    "        if self.apply_add_gauss:\n",
    "            dt_params = self.add_gauss(dt_params, self.guass_add_stds)\n",
    "        if self.apply_mult_gauss:\n",
    "            dt_params[...,3] = self.mult_gauss(dt_params[...,3])\n",
    "        \n",
    "        return dt_params, energy_labels\n",
    "\n",
    "    def __iter__(self):\n",
    "        \"\"\"\n",
    "        An iterator that yields batches of data.\n",
    "    \n",
    "        For training data, this creates an infinite dataset via self-loop.\n",
    "        For test data, this iterates once through the dataset.\n",
    "        \n",
    "        Yields:\n",
    "        - padded: Padded detector data with mask [batch_size, max_seq_len, 7]\n",
    "        - labels: Energy labels [batch_size, 1]\n",
    "        \"\"\"\n",
    "        \n",
    "        # Initialize multiple workers\n",
    "        # Get worker information\n",
    "        worker_info = torch.utils.data.get_worker_info()\n",
    "        # Determine the range of data this worker should process\n",
    "        if worker_info is None:  # single-process data loading\n",
    "            worker_start = 0\n",
    "            worker_end = self.stop\n",
    "        else:  # in a worker process\n",
    "            # Split workload \n",
    "            per_worker = int(np.ceil(self.stop / float(worker_info.num_workers)))\n",
    "            worker_id = worker_info.id\n",
    "            \n",
    "            worker_start = worker_id * per_worker\n",
    "            worker_end = min(worker_start + per_worker, self.stop)\n",
    "            \n",
    "            # Adjust to batch boundaries\n",
    "            worker_start = (worker_start // self.batch_size) * self.batch_size\n",
    "            worker_end = min(((worker_end + self.batch_size - 1) // self.batch_size) * self.batch_size, self.stop)\n",
    "        \n",
    "        # Open the HDF5 file within __iter__ so that each worker gets its own handle.\n",
    "        with h5.File(self.file, 'r') as hf:\n",
    "            start_ev = worker_start\n",
    "            \n",
    "            iterate = True\n",
    "            while iterate:\n",
    "                stop_ev = start_ev + self.batch_size\n",
    "                \n",
    "                # Check if we've reached the end of this worker's range\n",
    "                if stop_ev > worker_end:\n",
    "                    # For training, make infinite dataset by resetting to start\n",
    "                    if self.regime == 'train':\n",
    "                        # Reset back to the start of this worker's range\n",
    "                        start_ev = worker_start\n",
    "                        stop_ev = start_ev + self.batch_size\n",
    "                    else:\n",
    "                        iterate = False\n",
    "                \n",
    "                # Read detector indices for events\n",
    "                ev_idxs = hf[self.regime+'/ev_starts'][start_ev:stop_ev+1]\n",
    "                # Get detector parameters and energy labels\n",
    "                dt_params, labels = self.step(hf, start_ev, stop_ev, ev_idxs[0], ev_idxs[-1] )\n",
    "\n",
    "                # Make regular tensors\n",
    "                # Calculate the number of detectors per event\n",
    "                raw_lens = np.diff(ev_idxs).astype(np.int64)\n",
    "                max_len = raw_lens.max() # Maximum sequence length in this batch\n",
    "\n",
    "                # Convert the actual data to torch tensors.\n",
    "                data = torch.from_numpy(dt_params)      # shape: (total_dets, 6)\n",
    "                labels = torch.from_numpy(labels)       # shape: (total_evs, 1)\n",
    "                \n",
    "                # Create mask: 1 for real detectors, 0 for padding\n",
    "                mask = torch.ones((data.shape[0], 1), dtype=torch.float32) # shape: (total_dets, 1)\n",
    "                # Concatenate detector features and mask\n",
    "                data = torch.cat([data, mask], dim=-1)\n",
    "                \n",
    "                # Preallocate padded tensor: shape (batch_size, max_len, 7), filled with default values.\n",
    "                padded = torch.tile(dense_def_vals, (labels.shape[0], max_len, 1))\n",
    "                \n",
    "                # Create a boolean mask with shape (batch_size, max_len).\n",
    "                # For each event, positions [0, raw_lens[i]) are True.\n",
    "                mask = np.arange(max_len)[None, :] < raw_lens[:, None]  # shape: (batch_size, max_len)\n",
    "                mask_tensor = torch.from_numpy(mask)\n",
    "                \n",
    "                # Fill the padded tensor with actual data\n",
    "                padded[mask_tensor] = data\n",
    "\n",
    "                # Move to next batch\n",
    "                start_ev += self.batch_size\n",
    "\n",
    "                yield padded.float(), labels.float() # also convert to float32\n",
    "\n",
    "def make_datasets(file, batch_size,\n",
    "                 apply_add_gauss, gauss_stds,\n",
    "                 apply_mult_gauss, mult_gauss_std):\n",
    "    \"\"\"\n",
    "    Create train and test datasets.\n",
    "    \n",
    "    Parameters:\n",
    "    - file: Path to the HDF5 file\n",
    "    - batch_size: Number of events per batch\n",
    "    - apply_add_gauss: Whether to apply additive Gaussian noise (for augmentation)\n",
    "    - gauss_stds: Standard deviations for additive noise\n",
    "    - apply_mult_gauss: Whether to apply multiplicative Gaussian noise\n",
    "    - mult_gauss_std: Standard deviation for multiplicative noise\n",
    "    \n",
    "    Returns:\n",
    "    - train_dataset: DataLoader for training\n",
    "    - test_dataset: DataLoader for testing\n",
    "    \"\"\"\n",
    "    # Create generators for train and test datasets\n",
    "    train_generator = DatasetGenerator(file, 'train', batch_size, False,\n",
    "                 apply_add_gauss, gauss_stds,\n",
    "                 apply_mult_gauss, mult_gauss_std)\n",
    "    test_generator = DatasetGenerator(file, 'test', batch_size, False,\n",
    "                 False, None,\n",
    "                 False, None)\n",
    "\n",
    "    # Create DataLoader objects\n",
    "    train_dataset = DataLoader(train_generator, batch_size=None, shuffle=False, pin_memory=True, num_workers=0, prefetch_factor=None)\n",
    "    test_dataset = DataLoader(test_generator, batch_size=None, shuffle=False, pin_memory=True, num_workers=0, prefetch_factor=None)\n",
    "\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take a look at data\n",
    "\n",
    "Below we initiate dataset generator and take a look at one batch.\n",
    "Each event is padded to the maximal \"length\" in the batch and auxiliary detectors are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "  'file' : '../../../taml2025_data/taml_0325_energy.h5', # path to training file\n",
    "  'batch_size' : 4, # batch size\n",
    "  'apply_add_gauss' : False, # flag for addative data augmentation\n",
    "  'gauss_stds' : [0., 0., 0., 0., 0. , 0.], # noise parameters\n",
    "  'apply_mult_gauss' : False, # flag for multiplicative augmentation of registered charges\n",
    "  'mult_gauss_std' : 0.0 # noise parameters\n",
    "}\n",
    "\n",
    "train_generator = DatasetGenerator(regime='train', return_reminder=False, **generator_config)\n",
    "\n",
    "for data, label in train_generator.__iter__():\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 34, 7]) torch.Size([4, 1])\n",
      "tensor([[[ 0.5414, -1.5941, -1.2051, -0.4940, -0.5042, -0.3481,  1.0000],\n",
      "         [ 0.5461, -0.8243, -1.1074, -0.4972, -0.2328, -0.7657,  1.0000],\n",
      "         [-0.1986, -0.7951, -1.0037, -0.4665, -0.0465, -0.6553,  1.0000],\n",
      "         [ 0.5435, -0.0488, -1.0852,  0.0252,  0.0443, -0.8896,  1.0000],\n",
      "         [ 0.5435, -0.0488, -1.0852,  0.0252,  0.0443,  0.3297,  1.0000],\n",
      "         [ 0.5442,  0.7246, -0.9282, -0.4936,  0.3160, -0.7222,  1.0000],\n",
      "         [-0.9999, -0.0454, -1.0370, -0.4302,  0.4144, -0.9027,  1.0000],\n",
      "         [-0.2268,  0.7237, -0.8708, -0.3854,  0.4993, -0.7661,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]],\n",
      "\n",
      "        [[-1.7073, -0.1596, -0.5008, -0.4781, -1.1907, -0.6759,  1.0000],\n",
      "         [-1.7073, -0.1596, -0.5008, -0.4781, -1.1907,  0.3376,  1.0000],\n",
      "         [-0.9377, -1.0043, -0.6628, -0.4801, -0.7227, -0.4807,  1.0000],\n",
      "         [-0.9377, -1.0043, -0.6628, -0.4801, -0.7227,  0.5328,  1.0000],\n",
      "         [-0.9682, -0.1480, -0.4024, -0.2129, -0.6126, -0.8489,  1.0000],\n",
      "         [-0.9682, -0.1480, -0.4024, -0.2129, -0.6126,  0.4259,  1.0000],\n",
      "         [-0.9682, -0.1480, -0.4024, -0.2129, -0.6126,  1.6294,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606, -0.7372,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606,  0.4346,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606,  1.4480,  1.0000],\n",
      "         [-0.9341,  0.6097, -0.4815, -0.2790, -0.4606,  2.4615,  1.0000],\n",
      "         [-0.1643, -2.4843, -0.6656, -0.4904, -0.3595,  0.5280,  1.0000],\n",
      "         [-0.9321,  2.1536, -0.4799, -0.4921, -0.2065, -0.6672,  1.0000],\n",
      "         [-0.1642, -0.9376, -0.5772, -0.2701, -0.1087, -0.8631,  1.0000],\n",
      "         [-0.1642, -0.9376, -0.5772, -0.2701, -0.1087,  0.4670,  1.0000],\n",
      "         [-0.1962,  0.6206, -0.5225,  0.6251,  0.1194, -0.8892,  1.0000],\n",
      "         [-0.1962,  0.6206, -0.5225,  0.6251,  0.1194,  0.1242,  1.0000],\n",
      "         [-0.1612,  1.3805, -0.5596, -0.4496,  0.2721, -0.4134,  1.0000],\n",
      "         [-0.1612,  1.3805, -0.5596, -0.4496,  0.2721,  0.6001,  1.0000],\n",
      "         [ 0.6062, -1.7113, -0.7420, -0.4982,  0.3715, -0.0330,  1.0000],\n",
      "         [ 0.6078, -0.9393, -0.6400, -0.3373,  0.4966, -0.7123,  1.0000],\n",
      "         [ 0.6078, -0.9393, -0.6400, -0.3373,  0.4966,  0.3803,  1.0000],\n",
      "         [ 0.6094, -0.1638, -0.6697,  1.5633,  0.6250, -0.8972,  1.0000],\n",
      "         [ 0.6094, -0.1638, -0.6697,  1.5633,  0.6250,  0.1163,  1.0000],\n",
      "         [ 0.6416,  0.6082, -0.5930, -0.0728,  0.7747, -0.8378,  1.0000],\n",
      "         [ 0.6416,  0.6082, -0.5930, -0.0728,  0.7747,  0.1994,  1.0000],\n",
      "         [ 0.6416,  0.6082, -0.5930, -0.0728,  0.7747,  2.1313,  1.0000],\n",
      "         [ 1.3792, -0.9402, -0.5980, -0.4531,  1.0989, -0.3920,  1.0000],\n",
      "         [ 1.3792, -0.9402, -0.5980, -0.4531,  1.0989,  0.6927,  1.0000],\n",
      "         [ 1.3808, -0.1990, -0.7767, -0.4667,  1.2253, -0.6616,  1.0000],\n",
      "         [ 1.3845,  0.6058, -0.7530, -0.4715,  1.3590, -0.6466,  1.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "         [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]])\n"
     ]
    }
   ],
   "source": [
    "print(data.shape, label.shape)\n",
    "print(data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Metrics\n",
    "\n",
    "When training neural networks, we need metrics to track performance. \n",
    "Here we implement a simple metric that tracks the average value of the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanMetrics(nn.Module):\n",
    "\n",
    "    # Initialize metric\n",
    "    def __init__(self, name):\n",
    "        super().__init__()\n",
    "        self.register_buffer('value', torch.tensor(0.0))\n",
    "        self.register_buffer('steps', torch.tensor(0.0))\n",
    "        self.name = name\n",
    "\n",
    "    # Define how to reset metric\n",
    "    def reset(self):\n",
    "        self.value.zero_()\n",
    "        self.steps.zero_()\n",
    "\n",
    "    # Define how to update state\n",
    "    def update_state(self, value):\n",
    "        self.value.add_(value.detach())\n",
    "        self.steps.add_(1.)\n",
    "\n",
    "    # Define the resulting metric value\n",
    "    def result(self):\n",
    "        return self.value / (self.steps + 1e-9) # Add small epsilon to avoid division by zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Neural netowrk architecture\n",
    "\n",
    "Now we'll implement a transformer-based architecture for analyzing detector data. Transformers use self-attention mechanisms to weigh the importance of different detectors for updating nodes (detectors features) and making predictions.\n",
    "\n",
    "We will use several encoder layers to process data and then aggragate it to infer the energy of the primary particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Attention mechanism\n",
    "\n",
    "The core of a transformer architecture.\n",
    "It defines how each detector to \"looks at\" all other detectors in the event and gather information from those that are most relevant. \n",
    "For example:\n",
    "\n",
    "- A detector near the core of the shower might need to pay attention to distant detectors to understand the overall shower geometry\n",
    "- Detectors with unusual timing might need to be contextualized by the overall pattern\n",
    "- Some detectors might contain noise and should be downweighted\n",
    "\n",
    "The attention mechanism learns which detectors should \"pay attention\" to which other detectors to best reconstruct the energy of the primary particle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Query, Key, Value Projections\n",
    "\n",
    "**What are Query, Key, and Value?**\n",
    "\n",
    "The Query, Key, and Value (QKV) concepts come from information retrieval systems:\n",
    "\n",
    "*Query (Q)*: Represents \"what information am I looking for?\" For each detector, its Query representation encodes what kind of information it needs from other detectors.\n",
    "\n",
    "*Key (K)*: Represents \"what information do I contain?\" For each detector, its Key representation encodes what kind of information it can provide to other detectors.\n",
    "\n",
    "*Value (V)*: Represents \"what is my actual content?\" For each detector, its Value representation encodes the information it will contribute if deemed relevant.\n",
    "\n",
    "Query, Key, and Value are usually linear projections from nodes (detector) features. \n",
    "Such dimensin reduction is efficient and allows to control the required memory size.\n",
    "\n",
    "In general, Query, Key, and Value can be obtained from different sources (sequences).\n",
    "In our case, the source is the same - detectors data.\n",
    "Such attention mechanism is dubbed **self-attention**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# general implementation\n",
    "class QKVProjector(nn.Module):\n",
    "\n",
    "    def __init__(self, query_dim, key_dim, value_dim, att_dim):\n",
    "        \"\"\"\n",
    "        - query_dim, key_dim, value_dim: Dimension of query, key, values\n",
    "        - att_dim: Dimension for attention features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.q_proj = nn.Linear(query_dim, att_dim, bias=True)\n",
    "        self.k_proj = nn.Linear(key_dim, att_dim, bias=True)\n",
    "        self.v_proj = nn.Linear(value_dim, att_dim, bias=True)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.xavier_normal_(self.q_proj.weight)\n",
    "        nn.init.zeros_(self.q_proj.bias)\n",
    "        nn.init.xavier_normal_(self.k_proj.weight)\n",
    "        nn.init.zeros_(self.k_proj.bias)\n",
    "        nn.init.xavier_normal_(self.v_proj.weight)\n",
    "        nn.init.zeros_(self.v_proj.bias)\n",
    "\n",
    "    def forward(self, q_soure, k_source, v_source):\n",
    "        \"\"\"\n",
    "        Project input features to Q, K, V space.\n",
    "        \n",
    "        Parameters:\n",
    "        - query_soure: Input tensor [batch_size, seq_len, query_dim]\n",
    "        - key_soure: Input tensor [batch_size, seq_len, key_dim]\n",
    "        - value_soure: Input tensor [batch_size, seq_len, value_dim]\n",
    "        \n",
    "        Returns:\n",
    "        - q, k, v: Query, Key, Value projections [batch_size, seq_len, att_dim]\n",
    "        \"\"\"\n",
    "        qs = self.q_proj()\n",
    "        ks = self.k_proj()\n",
    "        vs = self.v_proj()\n",
    "        return qs, ks, vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# self-attenttion impementation\n",
    "class QKVProjector(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, att_dim):\n",
    "        \"\"\"\n",
    "        - input_dim: Dimension of input features\n",
    "        - att_dim: Dimension for attention features\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Single layer for self attention\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * att_dim, bias=True)\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_normal_(self.qkv_proj.weight)\n",
    "        nn.init.zeros_(self.qkv_proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Project input features to Q, K, V space.\n",
    "        \n",
    "        Parameters:\n",
    "        - x: Input tensor [batch_size, seq_len, input_dim]\n",
    "        \n",
    "        Returns:\n",
    "        - q, k, v: Query, Key, Value projections [batch_size, seq_len, att_dim]\n",
    "        \"\"\"\n",
    "        qkv = self.qkv_proj(x)\n",
    "        # Split into Q, K, V\n",
    "        return qkv.chunk(3, dim=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12ce967",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fbbaf47",
   "metadata": {},
   "outputs": [],
   "source": [
    "qkv_proj = QKVProjector(input_dim=7, att_dim=5)\n",
    "with torch.no_grad():\n",
    "    qs, ks, vs = qkv_proj(data)\n",
    "print(qs.shape, ks.shape, vs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Attention Layer\n",
    "\n",
    "Once we have the Q, K, and V representations, the attention layer computes how much each detector should attend to every other detector.\n",
    "\n",
    "The Attention Calculation Process:\n",
    "\n",
    "- Compatibility Scores: We compute the dot product between each Query and all Keys. If a detector's Query is similar to another detector's Key, they'll have a high compatibility score, meaning the first detector should pay attention to the second.\n",
    "- Scaling: We divide by √(head_dim) to prevent the dot products from growing too large, which would push the softmax into regions with very small gradients.\n",
    "- Masking: We apply a mask to zero out attention to padding elements (auxiliary detectors). We set their scores to a large negative value (-1e9) so that after softmax, they'll have practically zero weight.\n",
    "- Softmax Normalization: We apply softmax to convert the scores into a probability distribution (all positive, summing to 1).\n",
    "- Weighted Sum: We use these attention weights to compute a weighted sum of the Value vectors. This gives each detector a new representation that incorporates information from other relevant detectors.\n",
    "\n",
    "### Mask Shape and Creation\n",
    "\n",
    "The mask must cover the entire attention matrix, which has shape [batch_size, num_heads, seq_len, seq_len]. \n",
    "Each position (i,j) in this matrix represents the attention score from detector i to detector j.\n",
    "\n",
    "We will create proper mask later as it can be reused by all attention layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim):\n",
    "        \"\"\"\n",
    "        - head_dim: Dimension of each attention head\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Scaling factor for dot product attention (improves gradient stability)\n",
    "        scaling_factor = torch.sqrt(torch.tensor([head_dim], dtype=torch.float))\n",
    "        self.register_buffer('scaling_factor', scaling_factor)\n",
    "\n",
    "    def forward(self, qs, ks, vs, mask):\n",
    "        \"\"\"\n",
    "        Calculate attention scores and weighted values.\n",
    "        \n",
    "        Parameters:\n",
    "        - qs: Query tensor [batch_size, num_heads, seq_len, head_dim]\n",
    "        - ks: Key tensor [batch_size, num_heads, seq_len, head_dim]\n",
    "        - vs: Value tensor [batch_size, num_heads, seq_len, head_dim]\n",
    "        - mask: Attention mask to ignore padding [batch_size, 1, seq_len, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "        - Weighted sum of values based on attention scores\n",
    "        \"\"\"\n",
    "        # Calculate dot product attention scores\n",
    "        # (Q·K^T)/sqrt(d_k)\n",
    "        attention_logits = torch.matmul(qs, ks.transpose(-2, -1)) / self.scaling_factor\n",
    "         # Mask out padding by setting attention logits to large negative value\n",
    "        # This ensures softmax will give ~0 probability to padding\n",
    "        attention_logits = attention_logits.masked_fill(mask == 0, -1e9)\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_logits, dim=-1)\n",
    "         # Zero out weights for padding (additional safety)\n",
    "        attention_weights = attention_weights * mask\n",
    "         # Compute weighted sum of values\n",
    "        return torch.matmul(attention_weights, vs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959c4d3e",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001e6bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_layer = AttentionLayer(3)\n",
    "mask = data[:,:,-1:]\n",
    "with torch.no_grad():\n",
    "    scores = att_layer(qs, ks, vs, mask)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 Multi-Head Attention\n",
    "Multi-head attention extends the basic attention mechanism by allowing the model to jointly attend to information from different representation subspaces\n",
    "\n",
    "Why Multiple Heads?\n",
    "\n",
    "Think of multi-head attention as having multiple \"perspectives\" or \"viewpoints\" from which to analyze the relationships between detectors:\n",
    "\n",
    "- Different Relationship Types: Some heads might focus on spatial relationships, others on timing patterns, others on energy deposition correlations.\n",
    "- Complementary Information: Each head can capture different aspects of the detector relationships, and together they provide a more comprehensive understanding.\n",
    "- Parallel Processing: Multiple heads process information in parallel, allowing the model to capture multiple types of dependencies simultaneously.\n",
    "\n",
    "How Multi-Head Attention Works:\n",
    "\n",
    "- Projection: We project the input into Q, K, V representations that contain information for all heads concatenated.\n",
    "- Reshaping: We reshape these projections to separate the different heads.\n",
    "- Attention per Head: We apply the attention mechanism separately for each head.\n",
    "- Concatenation: We reshape and concatenate the outputs from all heads.\n",
    "- Final Projection: We project the concatenated output back to the model dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement multi head attention\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, num_heads, head_dim, out_features, dropout):\n",
    "        \"\"\"\n",
    "        - in_features: Input feature dimension\n",
    "        - num_heads: Number of attention heads\n",
    "        - head_dim: Dimension of each head\n",
    "        - out_features: Output feature dimension\n",
    "        - dropout: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        # Projection for Query, Key, Value\n",
    "        self.proj_layer = QKVProjector(in_features, num_heads * head_dim)\n",
    "        # Attention calculation layer\n",
    "        self.att_layer = AttentionLayer(head_dim)\n",
    "        # Final projection to output dimension\n",
    "        self.proj_out = nn.Linear(num_heads * head_dim, out_features, bias=True)\n",
    "        # Optional dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize output projection\n",
    "        nn.init.xavier_normal_(self.proj_out.weight)\n",
    "        nn.init.zeros_(self.proj_out.bias)\n",
    "\n",
    "    def forward(self, x, mask_att):\n",
    "        \"\"\"\n",
    "         Parametrs:\n",
    "        - x: Input tensor [batch_size, seq_len, in_features]\n",
    "        - mask_att: Attention mask [batch_size, 1, seq_len, seq_len]\n",
    "        \n",
    "        Returns:\n",
    "        - Output tensor [batch_size, seq_len, out_features]\n",
    "        \"\"\"\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Get query, key, value projections for all heads\n",
    "        qs, ks, vs = self.proj_layer(x) # shape [batch, seq_len, num_heads * head_dim]\n",
    "        # Reshape to separate the heads\n",
    "        # [batch_size, seq_len, num_heads*head_dim] -> [batch_size, num_heads, seq_len, head_dim]\n",
    "        qs = qs.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        ks = ks.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        vs = vs.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Apply attention mechanism\n",
    "        msgs = self.att_layer(qs, ks, vs, mask_att) # shape [batch, num_heads, seq_len, head_dim]\n",
    "        # Reshape back: [batch_size, seq_len, num_heads*head_dim]\n",
    "        # [batch, num_heads, seq_len, head_dim] -> [batch, seq_len, num_heads * head_dim]\n",
    "        msgs = msgs.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        # Final projection and dropout\n",
    "        return self.dropout(self.proj_out(msgs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0178cdc7",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1797e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha_layer = MultiheadAttention(in_features=7, num_heads=2, head_dim=5, out_features=4, dropout=0.)\n",
    "mask_att = (mask * torch.transpose(mask, 1, 2)).unsqueeze(1)\n",
    "with torch.no_grad():\n",
    "    scores = mha_layer(data, mask_att)\n",
    "print(scores.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Feed forward layer \n",
    "\n",
    "The Feed-Forward Network (FFN) follows the attention mechanism. \n",
    "While attention allows detectors to exchange information with each other, the FFN processes this information independently for each detector position.\n",
    "\n",
    "**Theoretical Background**\n",
    "\n",
    "In transformer architectures, the Feed-Forward Network serves several important purposes:\n",
    "\n",
    "- Feature Transformation: It applies non-linear transformations to the attention outputs, enabling the model to learn complex patterns\n",
    "- Position-wise Processing: Each detector's representation is processed independently, allowing the model to extract node-specific features\n",
    "- Capacity Enhancement: By expanding to a higher dimension and then projecting back, it increases the model's representational capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_ff, d_model, dropout=0.):\n",
    "        \"\"\"\n",
    "        Initialize the feed-forward network.\n",
    "        \n",
    "        Parameters:\n",
    "        - d_ff: Hidden layer dimension (typically 4x d_model for transformers)\n",
    "        - d_model: Model dimension (input and output)\n",
    "        - dropout: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # First linear transformation (expansion)\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        # Second linear transformation (projection)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The process follows: expand -> activate -> dropout -> project\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e18bdb0",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7f6f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_layer = FeedForward(14, 7, dropout=0.)\n",
    "with torch.no_grad():\n",
    "    updated = ff_layer(data)\n",
    "print(updated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Encoder Layer\n",
    "\n",
    "The Encoder Layer is the fundamental building block of the transformer architecture. \n",
    "It processes detector information through two main sub-layers: a multi-head self-attention mechanism and a position-wise feed-forward network. \n",
    "Multiple encoder layers are stacked to form the complete transformer encoder.\n",
    "\n",
    "**Theoretical Background**\n",
    "\n",
    "Each encoder layer performs the following operations:\n",
    "\n",
    "- Self-Attention: Allows each detector to attend to all other detectors, capturing relationships regardless of their spatial positions\n",
    "- Residual Connection + Layer Normalization: Helps with gradient flow and training stability\n",
    "- Feed-Forward Network: Processes each detector's representation independently\n",
    "- Second Residual Connection + Layer Normalization: Further improves training dynamics\n",
    "\n",
    "The residual connections (also called \"skip connections\") are crucial as they allow gradients to flow directly through the network during backpropagation, mitigating the vanishing gradient problem in deep networks.\n",
    "\n",
    "**The mathematical formulation for an encoder layer is:**\n",
    "\n",
    "AttentionBlock: z = LayerNorm(x + Dropout(MultiHeadAttention(x)))\n",
    "\n",
    "FFNBlock: output = LayerNorm(z + Dropout(FFN(z)))\n",
    "\n",
    "Each node (detector) are processed in parallel. \n",
    "Masking is applied to ignore padding detectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Single transformer encoder layer.\n",
    "    \n",
    "    Each encoder layer consists of:\n",
    "    1. Multi-head self-attention mechanism\n",
    "    2. Residual connection and layer normalization\n",
    "    3. Position-wise feed-forward network\n",
    "    4. Second residual connection and layer normalization\n",
    "    \"\"\"\n",
    "    def __init__(self, num_heads, d_model, d_ff, att_dim, dropout):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_heads: Number of attention heads (each focusing on different aspects)\n",
    "        - d_model: Model dimension (size of detector representations)\n",
    "        - d_ff: Feed-forward hidden dimension (typically 4x d_model)\n",
    "        - att_dim: Attention head dimension (d_model / num_heads)\n",
    "        - dropout: Dropout rate for regularization\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Multi-head attention for processing detector relationships\n",
    "        # This allows detectors to exchange information based on relevance\n",
    "        self.mha = MultiheadAttention(\n",
    "            in_features=d_model,\n",
    "            num_heads=num_heads,\n",
    "            head_dim=att_dim,\n",
    "            out_features=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Position-wise feed-forward network\n",
    "        # Processes each detector's features independently\n",
    "        self.ffn = FeedForward(d_ff, d_model, dropout)\n",
    "        # Layer normalization for training stability\n",
    "        # Applied after each residual connection\n",
    "        # Normalizes each detector's features to have zero mean and unit variance\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Dropout for regularization\n",
    "        # Applied to attention outputs and FFN outputs before residual connections\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask, mask_att):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - x: Input tensor [batch_size, seq_len, d_model]\n",
    "          Each element in seq_len represents a detector's features\n",
    "        - mask: mask for auxiliary detectors [batch_size, seq_len, 1]\n",
    "          Binary mask marking real (1) vs. padding (0) detectors\n",
    "        - mask_att: Attention mask [batch_size, 1, seq_len, seq_len]\n",
    "          Controls which detector pairs can attend to each other\n",
    "        \n",
    "        Returns:\n",
    "        - Output tensor [batch_size, seq_len, d_model]\n",
    "          Updated detector representations after attention and FFN\n",
    "        \"\"\"\n",
    "        # 1. Self-attention block\n",
    "        # Each detector gathers information from other relevant detectors\n",
    "        attn_output = self.mha(x, mask_att)\n",
    "        # 2. First residual connection and layer normalization\n",
    "        # Residual: Add the original input to preserve information\n",
    "        # LayerNorm: Normalize for stability\n",
    "        out1 = self.norm1(x + self.dropout(attn_output))\n",
    "        # 3. Feed-forward block with mask applied\n",
    "        # Process each detector independently and mask out padding detectors\n",
    "        ff_output = self.ffn(out1) * mask\n",
    "        # 4. Second residual connection and layer normalization.\n",
    "        # Apply mask to ensure padding detectors remain at zero\n",
    "        out2 = self.norm2(out1 + self.dropout(ff_output)) * mask\n",
    "        return out2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb6803a",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = EncoderLayer(num_heads=2, d_model=7, d_ff=14, att_dim=5, dropout=0.)\n",
    "with torch.no_grad():\n",
    "    updated_data = encoder_layer(data, mask, mask_att)\n",
    "print(updated_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Aggregation Layer\n",
    "\n",
    "To infer energy of the primary particle, we need to aggregate data from all detectors. \n",
    "We will use averaging for this purpose. \n",
    "\n",
    "Another approach is to introduce classifiaction token.\n",
    "We will not use this advanced technique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AggregateLayer(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\" \n",
    "        Return average taking into account mask\n",
    "        Parameters:\n",
    "        - x: data tensor [batch_size, seq_len, d_model]\n",
    "        - mask: mask for auxiliary detectors [batch_size, 1]\n",
    "        \"\"\"\n",
    "        return torch.sum(x*mask, dim=1) / torch.sum(mask, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeda371e",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3090f215",
   "metadata": {},
   "outputs": [],
   "source": [
    "aggr_layer = AggregateLayer()\n",
    "with torch.no_grad():\n",
    "    aggregated = aggr_layer(updated_data, mask)\n",
    "print(aggregated.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Prediction Layer\n",
    "\n",
    "The Prediction Layer is the final component of our network that takes the aggregated detector information and transforms it into the energy prediction.\n",
    "After the transformer encoder processes and contextualizes all detector activations, we need a way to combine this information and map it to our target variable: the log10 of the primary particle's energy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Final prediction layer that processes aggregated detector data.\n",
    "    \n",
    "    This layer implements a multi-layer perceptron (MLP) that maps from\n",
    "    the transformer's high-dimensional representation space to the \n",
    "    energy prediction (log10 scale). It consists of:\n",
    "    \n",
    "    1. A reduction layer that maps from the model dimension to a smaller dimension\n",
    "    2. Optional intermediate layers for additional capacity\n",
    "    3. A final output layer that produces the energy prediction\n",
    "    \n",
    "    Each layer is followed by a Leaky ReLU activation, except the final output\n",
    "    which produces a raw scalar value.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim_in, dim_middle, dim_out, num_middle_layers):\n",
    "        \"\"\"\n",
    "        - dim_in: Input dimension (from transformer encoder, typically d_model)\n",
    "        - dim_middle: Hidden layer dimension (smaller than dim_in for efficiency)\n",
    "        - dim_out: Output dimension (1 for energy prediction)\n",
    "        - num_middle_layers: Number of hidden layers between reduction and output\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Initial dimension reduction\n",
    "        # This compresses the high-dimensional detector representation\n",
    "        # to a more manageable size for the regression task\n",
    "        self.reduce = nn.Linear(dim_in, dim_middle)\n",
    "        # Hidden layers for additional modeling capacity\n",
    "        # Each layer maintains the same dimension (dim_middle)\n",
    "        self.pre_layers = nn.ModuleList([\n",
    "            nn.Linear(dim_middle, dim_middle) for _ in range(num_middle_layers)\n",
    "        ])\n",
    "        # Final output layer that produces the energy prediction\n",
    "        # Maps from the hidden dimension to a single scalar output\n",
    "        self.out = nn.Linear(dim_middle, dim_out)\n",
    "        # Leaky ReLU activation function\n",
    "        self.activation = F.leaky_relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - x: Input tensor [batch_size, dim_in]\n",
    "          This contains aggregated information from all detectors for each event\n",
    "        \n",
    "        Returns:\n",
    "        - Predictions [batch_size, dim_out]\n",
    "          Log10 of the predicted energy for each event\n",
    "        \"\"\"\n",
    "        # Initial dimension reduction with activation\n",
    "        x = self.reduce(x)\n",
    "        x = self.activation(x)\n",
    "        # Apply each hidden layer with activation\n",
    "        for layer in self.pre_layers:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        # Final prediction layer (no activation)\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f921d221",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb73820f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_layer = PredictLayer(dim_in=7, dim_middle=2, dim_out=1, num_middle_layers=2)\n",
    "with torch.no_grad():\n",
    "    preds = pred_layer(aggregated)\n",
    "print(preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Embedding layer\n",
    "\n",
    "Our initial data has 6 features, while transformer has higher dimensionality.\n",
    "We need to embed your data to this higher dimensional space.\n",
    "\n",
    "This can be done via a single linear layer.\n",
    "To preserve physical data, we initialize it as identity matrix and allow NN to optimize it.\n",
    "\n",
    "### On positional encoding\n",
    "\n",
    "Position encoding is needed when node encodings do not provide positional information (for example, tokens in natural language processing).\n",
    "In our case, nodes (detectors) has both temporal and spatial information, which yield positional encoding redundant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_out):\n",
    "        \"\"\"\n",
    "        - dim_in: Input dimension (number of detector features)\n",
    "        - dim_out: Hidden layer dimension (transformer dimensionality)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.dense = nn.Linear(dim_in, dim_out, bias=False)\n",
    "        # Initialize matrix as identity\n",
    "        nn.init.eye_(self.dense.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Linearly map to transformer dimensionality \n",
    "        return self.dense(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43971a",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c68fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_layer = EmbeddingLayer(dim_in=7, dim_out=24)\n",
    "with torch.no_grad():\n",
    "    embd = emb_layer(data)\n",
    "print(embd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Complete Encoder Model\n",
    "\n",
    "The Encoder class brings together all the components we've built to create a complete transformer-based model for cosmic ray energy reconstruction. This model embodies the full data processing pipeline from raw detector activations to energy predictions.\n",
    "\n",
    "The model architecture follows this sequence:\n",
    "- Create a mask for auxiliary detectors\n",
    "- Embedding: Project raw detector features into a higher-dimensional representation space\n",
    "- Transformer Layers: Process data through multiple encoder layers\n",
    "- Aggregation: Combine information from all detectors into a single event representation\n",
    "- Prediction: Generate the final energy prediction from the aggregated representation\n",
    "\n",
    "We will create special function for creating mask which will be used in attention layer.\n",
    "For this purpose, will follow these steps:\n",
    "- Start with a 1D mask of shape [batch_size, seq_len, 1], in which 1 indicates a real detector and 0 indicates padding\n",
    "- Create a 2D mask by multiplying this vector with its transpose: mask * mask.transpose(1,2)\n",
    "- This gives a matrix of shape [batch_size, seq_len, seq_len] where position (i,j) is 1 only if both detector i and j are real\n",
    "- Expand this to include the heads dimension: [batch_size, 1, seq_len, seq_len]. This dimensional expasion is required for proper broadcasting with multi head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, head_dim, input_dim, dropout,\n",
    "                    dim_middle_pred, dim_out_pred, num_middle_layers_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_layers: Number of stacked encoder layers (depth of the model)\n",
    "        - num_heads: Number of attention heads in each encoder layer\n",
    "        - d_model: Model dimension - internal representation size for detector features\n",
    "        - d_ff: Feed-forward hidden dimension (typically 4x d_model)\n",
    "        - head_dim: Dimension of each attention head\n",
    "        - input_dim: Input feature dimension (6 detector features + 1 mask)\n",
    "        - dropout: Dropout rate for regularization\n",
    "        - dim_middle_pred: Hidden dimension in the prediction layer\n",
    "        - dim_out_pred: Output dimension (1 for energy prediction)\n",
    "        - num_middle_layers_pred: Number of hidden layers in prediction network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # Embedding layer: Projects the 6 detector features into the model dimension\n",
    "        # We use input_dim-1 because the last dimension is the mask\n",
    "        self.embedding_layer = EmbeddingLayer(input_dim-1, d_model)\n",
    "        # Stack of encoder layers: Each layer processes the detector information\n",
    "        # and refines the representation through self-attention and feed-forward networks\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(num_heads, d_model, d_ff, head_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        # Aggregation layer: aggragate data from all detectors.\n",
    "        # It will be used to infer log10 energyof the primary particle.\n",
    "        self.aggr_layer = AggregateLayer()\n",
    "        # Prediction layer: Takes the aggregated detector information and\n",
    "        # predicts the energy of the primary particle\n",
    "        self.predict_layer = PredictLayer(d_model, dim_middle_pred, dim_out_pred, num_middle_layers_pred)\n",
    "\n",
    "    def compile(self, optim_kwargs, scheduler_kwargs):\n",
    "        \"\"\"\n",
    "        Configure the model for training by defining loss function, optimizer,\n",
    "        learning rate scheduler, and evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        - optim_kwargs: Optimizer parameters\n",
    "        - scheduler_kwargs: Learning rate scheduler parameters\n",
    "        \"\"\"\n",
    "        # Define loss function - Mean Squared Error for regression task\n",
    "        self.loss = nn.functional.mse_loss\n",
    "        # Configure Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), **optim_kwargs)\n",
    "        # Learning rate scheduler that reduces LR when performance plateaus\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **scheduler_kwargs)\n",
    "        # Define metrics to track during training\n",
    "        metric_names = ['mse_logE_loss']\n",
    "        self.metrics = [ MeanMetrics(name=mn) for mn in metric_names ]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"  \n",
    "        The data flow follows these steps:\n",
    "        1. Split data into detector features and mask\n",
    "        2. Create attention mask to prevent attending to padding\n",
    "        3. Embed detector features into higher-dimensional space\n",
    "        4. Process through multiple encoder layers\n",
    "        5. Aggregate information from all detectors\n",
    "        6. Generate energy prediction\n",
    "        \n",
    "        Parameters:\n",
    "        - inputs: Input tensor with shape [batch_size, seq_len, input_dim]\n",
    "          where seq_len is the maximum number of detectors in the batch\n",
    "          and the last feature is the mask\n",
    "        \n",
    "        Returns:\n",
    "        - Energy predictions [batch_size, 1] in log10 scale\n",
    "        \"\"\"\n",
    "        # Split input into data features and mask\n",
    "        # x: [batch_size, seq_len, 6] - detector features\n",
    "        # mask: [batch_size, seq_len, 1] - binary mask\n",
    "        x, mask = inputs[:, :, :-1], inputs[:, :, -1:]\n",
    "        # Create 2D attention mask from the 1D feature mask\n",
    "        # This mask ensures that:\n",
    "        # - Real detectors can attend to other real detectors\n",
    "        # - Real detectors cannot attend to padding\n",
    "        # - Padding cannot attend to anything\n",
    "        mask_att = (mask * torch.transpose(mask, 1, 2)).unsqueeze(1) # (bs, 1, l, l)\n",
    "        # Embed input features into the model's higher-dimensional space\n",
    "        # This projection preserves physical meaning while allowing more expressive representations\n",
    "        x = self.embedding_layer(x)\n",
    "        # Process through the stack of encoder layers\n",
    "        # Each layer refines the detector representations\n",
    "        for enc_layer in self.enc_layers:\n",
    "            x = enc_layer(x, mask, mask_att)\n",
    "        # Aggregate data to a single vector\n",
    "        aggr = self.aggr_layer(x, mask) #torch.sum(x*mask, dim=1) / torch.sum(mask, dim=1)\n",
    "        # Generate energy prediction from the aggregated event representation\n",
    "        preds = self.predict_layer(aggr)\n",
    "        return preds\n",
    "\n",
    "    def update_metrics(self, metric_updates):\n",
    "        \"\"\"\n",
    "        Update tracking metrics with new values.\n",
    "        \n",
    "        Parameters:\n",
    "        - metric_updates: List of new metric values from the most recent step\n",
    "        \"\"\"\n",
    "        for m_update, m_tracker in zip(metric_updates, self.metrics):\n",
    "            m_tracker.update_state(m_update)\n",
    "\n",
    "    def train_step(self, data, labels):\n",
    "        \"\"\"\n",
    "        Perform a single training step with backpropagation.\n",
    "        \n",
    "        This method:\n",
    "        1. Sets the model to training mode\n",
    "        2. Performs forward pass\n",
    "        3. Calculates loss\n",
    "        4. Computes gradients via backpropagation\n",
    "        5. Updates model parameters\n",
    "        6. Updates metrics\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input detector data [batch_size, seq_len, input_dim+1]\n",
    "        - labels: True energy values [batch_size, 1] in log10 scale\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of metrics including loss and learning rate\n",
    "        \"\"\"\n",
    "        # Set model to training mode (enables dropout, batch norm updates, etc.)\n",
    "        self.train()\n",
    "        # Zero gradients from previous step\n",
    "        # This is necessary because PyTorch accumulates gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forward pass: Generate predictions\n",
    "        preds = self.forward(data)\n",
    "        # Calculate loss between predictions and true values\n",
    "        loss = self.loss(preds, labels)\n",
    "        # Backward pass: Compute gradient of loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Update weights using the optimizer\n",
    "        self.optimizer.step()\n",
    "        # Update tracking metrics\n",
    "        self.update_metrics([loss])\n",
    "        # Return metrics dictionary for loggin\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}\n",
    "\n",
    "    def test_step(self, data, labels):\n",
    "        \"\"\"\n",
    "        Perform a single validation/test step without parameter updates.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode (disables dropout, freezes batch norm, etc.)\n",
    "        self.eval()\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass: Generate predictions\n",
    "            preds = self.forward(data)\n",
    "            # Calculate loss between predictions and true values\n",
    "            loss = self.loss(preds, labels)\n",
    "        # Update tracking metrics\n",
    "        self.update_metrics([loss])\n",
    "        # Return metrics dictionary for logging\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0be348",
   "metadata": {},
   "source": [
    "### Test the layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee66b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(num_layers=3, num_heads=2, d_model=24, d_ff=48, head_dim=5, input_dim=7, dropout=0.,\n",
    "                    dim_middle_pred=12, dim_out_pred=1, num_middle_layers_pred=2)\n",
    "with torch.no_grad():\n",
    "    encoded = encoder(data)\n",
    "print(encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Preparing for Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Set various configurations\n",
    "\n",
    "### 5.1.1 Set configuration of datasets for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_config = {\n",
    "  'file' : '../../../taml2025_data/taml_0325_energy.h5', # path to training file\n",
    "  'batch_size' : 128, # batch size\n",
    "  'apply_add_gauss' : False, # flag for addative data augmentation\n",
    "  'gauss_stds' : [0., 0., 0., 0., 0. , 0.], # noise parameters\n",
    "  'apply_mult_gauss' : False, # flag for multiplicative augmentation of registered charges\n",
    "  'mult_gauss_std' : 0.0 # noise parameters\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.2 Set model configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural network architecture parameters\n",
    "nn_arch_params = {\n",
    "  'num_layers': 5,       # Number of transformer encoder layers\n",
    "  'num_heads': 4,        # Number of attention heads per layer\n",
    "  'd_model': 128,        # Model dimension\n",
    "  'd_ff': 512,           # Feed-forward hidden dimension (4 * d_model)\n",
    "  'head_dim': 32,        # Dimension of each attention head\n",
    "  'input_dim': 7,        # Input features dimension (6 + 1 mask)\n",
    "  'dropout': 0.,         # Dropout rate\n",
    "  'dim_middle_pred': 32, # Prediction hidden dimension\n",
    "  'dim_out_pred': 1,     # Output dimension (1 for energy)\n",
    "  'num_middle_layers_pred': 1  # Number of prediction hidden layers\n",
    "}\n",
    "\n",
    "optimizer_params ={\n",
    "  'lr': 0.0005  # Learning rate\n",
    "  }\n",
    "\n",
    "scheduler_params = {\n",
    "  'factor': 0.25,    # Factor to reduce learning rate by\n",
    "  'patience': 4      # Number of epochs with no improvement before reducing LR\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1.3 Set Training Configuration (including Early Stopping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "model_name = 'taml_test'            # Name for saving the model\n",
    "patience = 8                        # Early stopping patience\n",
    "train_steps_per_epoch = 500         # Number of batches per epoch\n",
    "test_steps_per_epoch = 100          # Number of test batches per epoch\n",
    "min_delta = 1.e-4                   # Minimum improvement for early stopping\n",
    "num_epochs = 10                     # Maximum number of epochs; increase for real training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Define training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(nn_arch_builder):\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset, test_dataset = make_datasets(**generator_config)\n",
    "\n",
    "    # Create model and move to GPU\n",
    "    model = nn_arch_builder(**nn_arch_params)\n",
    "    model.to(device)\n",
    "    # Compile the model\n",
    "    model.compile(optimizer_params, scheduler_params)\n",
    "\n",
    "    # Create infinite training data iterator\n",
    "    train_iter = iter(train_dataset)\n",
    "    \n",
    "    # Initialize early stopping variables\n",
    "    best_loss = 1.e9\n",
    "    wait = 0  # Counter for patience\n",
    "    \n",
    "    # Move metrics to GPU\n",
    "    for metric in model.metrics:\n",
    "        metric.to(device)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch+1}\")\n",
    "    \n",
    "        ### TRAINING PHASE\n",
    "        # Reset metrics\n",
    "        for metric in model.metrics:\n",
    "            metric.reset()\n",
    "    \n",
    "        # Train for specified number of steps\n",
    "        for step in range(train_steps_per_epoch):\n",
    "            # Get next batch from infinite iterator\n",
    "            batch = next(train_iter)\n",
    "            # Move data to GPU\n",
    "            data, labels = [b.to(device) for b in batch]\n",
    "            # Perform training step\n",
    "            all_metrics = model.train_step(data, labels)\n",
    "        # Print training metrics\n",
    "        print(f\"Train loss: {all_metrics['mse_logE_loss'].item()}\")\n",
    "    \n",
    "        ### VALIDATION PHASE\n",
    "        # Reset metrics\n",
    "        for metric in model.metrics:\n",
    "            metric.reset()\n",
    "    \n",
    "        # Validate on test dataset\n",
    "        for i, batch in enumerate(test_dataset):\n",
    "            if i >= test_steps_per_epoch:\n",
    "                break\n",
    "            data, labels = [b.to(device) for b in batch]\n",
    "            all_metrics = model.test_step(data, labels)\n",
    "            \n",
    "        print(f\"Test loss: {all_metrics['mse_logE_loss'].item()}\")\n",
    "    \n",
    "        # Early stopping check\n",
    "        val_loss = all_metrics['mse_logE_loss']\n",
    "        # EarlyStopping and ModelCheckpoint\n",
    "        if val_loss < best_loss - min_delta:\n",
    "            # We have improvement\n",
    "            best_loss = val_loss\n",
    "            wait = 0\n",
    "        else:\n",
    "            # No improvement\n",
    "            wait += 1\n",
    "            if wait >= patience:\n",
    "                print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                break\n",
    "    \n",
    "        # Update learning rate based on validation loss\n",
    "        model.scheduler.step(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Train loss: 0.05008336901664734\n",
      "Test loss: 0.012770221568644047\n",
      "Epoch 2\n",
      "Train loss: 0.009681055322289467\n",
      "Test loss: 0.008545896038413048\n",
      "Epoch 3\n",
      "Train loss: 0.008892541751265526\n",
      "Test loss: 0.007970643229782581\n",
      "Epoch 4\n",
      "Train loss: 0.008370478637516499\n",
      "Test loss: 0.010601428337395191\n",
      "Epoch 5\n",
      "Train loss: 0.008061196655035019\n",
      "Test loss: 0.007009638473391533\n",
      "Epoch 6\n",
      "Train loss: 0.007508471608161926\n",
      "Test loss: 0.007092840038239956\n",
      "Epoch 7\n",
      "Train loss: 0.00760198850184679\n",
      "Test loss: 0.006917542312294245\n",
      "Epoch 8\n",
      "Train loss: 0.007241031154990196\n",
      "Test loss: 0.0080404719337821\n",
      "Epoch 9\n",
      "Train loss: 0.0071657029911875725\n",
      "Test loss: 0.006560187321156263\n",
      "Epoch 10\n",
      "Train loss: 0.00663840863853693\n",
      "Test loss: 0.007683474570512772\n"
     ]
    }
   ],
   "source": [
    "train_model(Encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Native PyToch implementation of transformer-encoder\n",
    "\n",
    "PyTorch has a built-in layer for transformer encoder, which can be used directly.\n",
    "This provide less control over the architecture, but is convenient.\n",
    "Below the same encoder architecture is created using PyTorch native transformer-encoder layer.\n",
    "Note that masking has changed.\n",
    "\n",
    "In our custom implementation we can set dimensionlity of attention heads.\n",
    "In PyTorch their dimensionality is fixed as (d_model // num_heads)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderNative(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, head_dim, input_dim, dropout,\n",
    "                    dim_middle_pred, dim_out_pred, num_middle_layers_pred):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - num_layers: Number of stacked encoder layers (depth of the model)\n",
    "        - num_heads: Number of attention heads in each encoder layer\n",
    "          (allows the model to focus on different aspects of the data)\n",
    "        - d_model: Model dimension - internal representation size for detector features\n",
    "        - d_ff: Feed-forward hidden dimension (typically 4x d_model)\n",
    "        - head_dim: Dimension of each attention head (d_model / num_heads)\n",
    "        - input_dim: Input feature dimension (6 detector features + 1 mask)\n",
    "        - dropout: Dropout rate for regularization\n",
    "        - dim_middle_pred: Hidden dimension in the prediction layer\n",
    "        - dim_out_pred: Output dimension (1 for energy prediction)\n",
    "        - num_middle_layers_pred: Number of hidden layers in prediction network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        # Embedding layer: Projects the 6 detector features into the model dimension\n",
    "        # We use input_dim-1 because the last dimension is the mask\n",
    "        self.embedding_layer = EmbeddingLayer(input_dim-1, d_model)\n",
    "\n",
    "        # This is how to initialize PyToch Transformer layer\n",
    "        # PyTorch native transformer encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=d_ff,\n",
    "            dropout=dropout,\n",
    "            activation=\"gelu\",\n",
    "            batch_first=True,\n",
    "            norm_first=True\n",
    "        )       \n",
    "        self.transformer_encoder = nn.TransformerEncoder(\n",
    "            encoder_layer=encoder_layer,\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "               \n",
    "        # Aggregation layer: aggragate data from all detectors.\n",
    "        # It will be used to infer log10 energyof the primary particle.\n",
    "        self.aggr_layer = AggregateLayer()\n",
    "        # Prediction layer: Takes the aggregated detector information and\n",
    "        # predicts the energy of the primary particle\n",
    "        self.predict_layer = PredictLayer(d_model, dim_middle_pred, dim_out_pred, num_middle_layers_pred)\n",
    "\n",
    "    def compile(self, optim_kwargs, scheduler_kwargs):\n",
    "        \"\"\"\n",
    "        Configure the model for training by defining loss function, optimizer,\n",
    "        learning rate scheduler, and evaluation metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        - optim_kwargs: Optimizer parameters\n",
    "        - scheduler_kwargs: Learning rate scheduler parameters\n",
    "        \"\"\"\n",
    "        # Define loss function - Mean Squared Error for regression task\n",
    "        self.loss = nn.functional.mse_loss\n",
    "        # Configure Adam optimizer\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), **optim_kwargs)\n",
    "        # Learning rate scheduler that reduces LR when performance plateaus\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **scheduler_kwargs)\n",
    "        # Define metrics to track during training\n",
    "        metric_names = ['mse_logE_loss']\n",
    "        self.metrics = [ MeanMetrics(name=mn) for mn in metric_names ]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        \"\"\"  \n",
    "        The data flow follows these steps:\n",
    "        1. Split data into detector features and mask\n",
    "        2. Create attention mask to prevent attending to padding\n",
    "        3. Embed detector features into higher-dimensional space\n",
    "        4. Process through multiple encoder layers\n",
    "        5. Aggregate information from all detectors\n",
    "        6. Generate energy prediction\n",
    "        \n",
    "        Parameters:\n",
    "        - inputs: Input tensor with shape [batch_size, seq_len, input_dim]\n",
    "          where seq_len is the maximum number of detectors in the batch\n",
    "          and the last feature is the mask\n",
    "        \n",
    "        Returns:\n",
    "        - Energy predictions [batch_size, 1] in log10 scale\n",
    "        \"\"\"\n",
    "        # Split input into data features and mask\n",
    "        # x: [batch_size, seq_len, 6] - detector features\n",
    "        # mask: [batch_size, seq_len, 1] - binary mask\n",
    "        x, mask = inputs[:, :, :-1], inputs[:, :, -1:]\n",
    "        # Create 2D attention mask from the 1D feature mask\n",
    "        \n",
    "        # We need to cast mask to the form required by PyTorch\n",
    "        mask_bool = (mask[:,:,0] == 0) \n",
    "        mask_att = torch.tile(mask_bool.unsqueeze(-1) * torch.transpose(mask_bool.unsqueeze(-1), 1, 2), [self.num_heads,1,1]) # (batch_size*num_heads, l, l)\n",
    "        \n",
    "        # Embed input features into the model's higher-dimensional space\n",
    "        # This projection preserves physical meaning while allowing more expressive representations\n",
    "        x = self.embedding_layer(x)\n",
    "        # Process through the stack of encoder layers\n",
    "        # Each layer refines the detector representations\n",
    "        # Mask parameters prevent transformer for attending auxiliary detectors\n",
    "        x = self.transformer_encoder(src=x,\n",
    "            src_key_padding_mask=mask_bool,\n",
    "            mask = mask_att)\n",
    "        # Aggregate data to a single vector\n",
    "        aggr = self.aggr_layer(x, mask) #torch.sum(x*mask, dim=1) / torch.sum(mask, dim=1)\n",
    "        # Generate energy prediction from the aggregated event representation\n",
    "        preds = self.predict_layer(aggr)\n",
    "        return preds\n",
    "\n",
    "    def update_metrics(self, metric_updates):\n",
    "        \"\"\"\n",
    "        Update tracking metrics with new values.\n",
    "        \n",
    "        Parameters:\n",
    "        - metric_updates: List of new metric values from the most recent step\n",
    "        \"\"\"\n",
    "        for m_update, m_tracker in zip(metric_updates, self.metrics):\n",
    "            m_tracker.update_state(m_update)\n",
    "\n",
    "    def train_step(self, data, labels):\n",
    "        \"\"\"\n",
    "        Perform a single training step with backpropagation.\n",
    "        \n",
    "        This method:\n",
    "        1. Sets the model to training mode\n",
    "        2. Performs forward pass\n",
    "        3. Calculates loss\n",
    "        4. Computes gradients via backpropagation\n",
    "        5. Updates model parameters\n",
    "        6. Updates metrics\n",
    "        \n",
    "        Parameters:\n",
    "        - data: Input detector data [batch_size, seq_len, input_dim+1]\n",
    "        - labels: True energy values [batch_size, 1] in log10 scale\n",
    "        \n",
    "        Returns:\n",
    "        - Dictionary of metrics including loss and learning rate\n",
    "        \"\"\"\n",
    "        # Set model to training mode (enables dropout, batch norm updates, etc.)\n",
    "        self.train()\n",
    "        # Zero gradients from previous step\n",
    "        # This is necessary because PyTorch accumulates gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        # Forward pass: Generate predictions\n",
    "        preds = self.forward(data)\n",
    "        # Calculate loss between predictions and true values\n",
    "        loss = self.loss(preds, labels)\n",
    "        # Backward pass: Compute gradient of loss with respect to parameters\n",
    "        loss.backward()\n",
    "        # Update weights using the optimizer\n",
    "        self.optimizer.step()\n",
    "        # Update tracking metrics\n",
    "        self.update_metrics([loss])\n",
    "        # Return metrics dictionary for loggin\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}\n",
    "\n",
    "    def test_step(self, data, labels):\n",
    "        \"\"\"\n",
    "        Perform a single validation/test step without parameter updates.\n",
    "        \"\"\"\n",
    "        # Set model to evaluation mode (disables dropout, freezes batch norm, etc.)\n",
    "        self.eval()\n",
    "        # Disable gradient calculation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass: Generate predictions\n",
    "            preds = self.forward(data)\n",
    "            # Calculate loss between predictions and true values\n",
    "            loss = self.loss(preds, labels)\n",
    "        # Update tracking metrics\n",
    "        self.update_metrics([loss])\n",
    "        # Return metrics dictionary for logging\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anatoli/mbp14_def_312/lib/python3.12/site-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03762780874967575\n",
      "Test loss: 0.013057490810751915\n",
      "Epoch 2\n",
      "Train loss: 0.012408399023115635\n",
      "Test loss: 0.01148897036910057\n",
      "Epoch 3\n",
      "Train loss: 0.011236725375056267\n",
      "Test loss: 0.009870955720543861\n",
      "Epoch 4\n",
      "Train loss: 0.010214113630354404\n",
      "Test loss: 0.009806573390960693\n",
      "Epoch 5\n",
      "Train loss: 0.00949963741004467\n",
      "Test loss: 0.009398655034601688\n",
      "Epoch 6\n",
      "Train loss: 0.00906671304255724\n",
      "Test loss: 0.007963735610246658\n",
      "Epoch 7\n",
      "Train loss: 0.008783363737165928\n",
      "Test loss: 0.00794818252325058\n",
      "Epoch 8\n",
      "Train loss: 0.008540401235222816\n",
      "Test loss: 0.0091423150151968\n",
      "Epoch 9\n",
      "Train loss: 0.008437572047114372\n",
      "Test loss: 0.00783203262835741\n",
      "Epoch 10\n",
      "Train loss: 0.007968190126121044\n",
      "Test loss: 0.00795616302639246\n"
     ]
    }
   ],
   "source": [
    "train_model(EncoderNative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Compare NN performance to standard reconstruction\n",
    "\n",
    "We will load trained transofrmer-encoder to compare 68% energy resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 NN architecture\n",
    "\n",
    "We will need the code used for making the transformer. It is a bit different from the one presented above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QKVProjector(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, att_dim):\n",
    "        super().__init__()\n",
    "        # Single layer for self attention\n",
    "        self.qkv_proj = nn.Linear(input_dim, 3 * att_dim, bias=True)\n",
    "        # Initialize weights\n",
    "        nn.init.xavier_normal_(self.qkv_proj.weight)\n",
    "        nn.init.zeros_(self.qkv_proj.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        qkv = self.qkv_proj(x)\n",
    "        # Split into Q, K, V\n",
    "        return qkv.chunk(3, dim=-1)\n",
    "\n",
    "# Calculate attention weights\n",
    "class AttentionLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, head_dim):\n",
    "        super().__init__()\n",
    "        # For gradient propagations, it is desired to rescale input data \n",
    "        scaling_factor = torch.sqrt(torch.tensor([head_dim], dtype=torch.float))\n",
    "        self.register_buffer('scaling_factor', scaling_factor)\n",
    "\n",
    "    def forward(self, qs, ks, vs, mask):\n",
    "        # Calculate attention weights\n",
    "        attention_logits = torch.matmul(qs, ks.transpose(-2, -1)) / self.scaling_factor\n",
    "        # Ensure that auxiliary data is not attended to ( softmax(large negative) = 0 ) \n",
    "        attention_logits = attention_logits.masked_fill(mask == 0, -1e9)\n",
    "        attention_weights = torch.softmax(attention_logits, dim=-1)\n",
    "        # For safety, zero out attention weights for aux data\n",
    "        attention_weights = attention_weights * mask\n",
    "        # Return accumulated message for updates\n",
    "        return torch.matmul(attention_weights, vs)\n",
    "\n",
    "# Implement multi head attention\n",
    "class MultiheadAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_features, num_heads, head_dim, out_features, dropout):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.proj_layer = QKVProjector(in_features, num_heads * head_dim)\n",
    "        self.att_layer = AttentionLayer(head_dim)\n",
    "        # Layer for making single update message\n",
    "        self.proj_out = nn.Linear(num_heads * head_dim, out_features, bias=True)\n",
    "        # Optional dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        nn.init.xavier_normal_(self.proj_out.weight)\n",
    "        nn.init.zeros_(self.proj_out.bias)\n",
    "\n",
    "    def forward(self, x, mask_att):\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        # Get Q, K, V for all attention heads\n",
    "        qs, ks, vs = self.proj_layer(x) # shape [batch, seq_len, num_heads * head_dim]\n",
    "        # Split Q, K, V to heads\n",
    "        # [batch, seq_len, num_heads * head_dim] -> [batch, num_heads, seq_len, head_dim]\n",
    "        qs = qs.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        ks = ks.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        vs = vs.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        # Apply attention\n",
    "        msgs = self.att_layer(qs, ks, vs, mask_att) # shape [batch, num_heads, seq_len, head_dim]\n",
    "        # Reshape to form single message for updating\n",
    "        # [batch, num_heads, seq_len, head_dim] -> [batch, seq_len, num_heads * head_dim]\n",
    "        msgs = msgs.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "        # Form update message and apply dropout\n",
    "        return self.dropout(self.proj_out(msgs))\n",
    "\n",
    "### Transformer-Encoder\n",
    "\n",
    "# Feed Forward (internal updating)\n",
    "class FeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, d_ff, d_model, dropout=0.):\n",
    "        super().__init__()\n",
    "        # Two dense layers with dropout for updating\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        nn.init.xavier_normal_(self.linear1.weight)\n",
    "        nn.init.xavier_normal_(self.linear2.weight)\n",
    "        nn.init.zeros_(self.linear1.bias)\n",
    "        nn.init.zeros_(self.linear2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.gelu(self.linear1(x))))\n",
    "\n",
    "# Layer for estimating target parameter\n",
    "class PredictLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, dim_in, dim_middle, dim_out, num_middle_layers):\n",
    "        super().__init__()\n",
    "        # Layer to reduce dimensionality\n",
    "        self.reduce = nn.Linear(dim_in, dim_middle)\n",
    "        # Some dense layers to process data\n",
    "        self.pre_layers = nn.ModuleList([\n",
    "            nn.Linear(dim_middle, dim_middle) for _ in range(num_middle_layers)\n",
    "        ])\n",
    "        # Layer to output predictions\n",
    "        self.out = nn.Linear(dim_middle, dim_out)\n",
    "        # Non-linarity to use\n",
    "        self.activation = F.leaky_relu\n",
    "\n",
    "    def forward(self, x):\n",
    "        # First layer explicitly\n",
    "        x = self.reduce(x)\n",
    "        x = self.activation(x)\n",
    "        # Middle layers via loop\n",
    "        for layer in self.pre_layers:\n",
    "            x = layer(x)\n",
    "            x = self.activation(x)\n",
    "        return self.out(x)\n",
    "\n",
    "# Single encoder layer\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, num_heads, d_model, d_ff, att_dim, dropout):\n",
    "        super().__init__()\n",
    "        # Multi head attention\n",
    "        self.mha = MultiheadAttention(\n",
    "            in_features=d_model,\n",
    "            num_heads=num_heads,\n",
    "            head_dim=att_dim,\n",
    "            out_features=d_model,\n",
    "            dropout=dropout\n",
    "        )\n",
    "        # Feed forward (internal updating)\n",
    "        self.ffn = FeedForward(d_ff, d_model, dropout)\n",
    "        # Normalization (for improved gradient propagation)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        # Optional regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask, mask_att):\n",
    "        # Get update messaged from attention\n",
    "        attn_output = self.mha(x, mask_att)\n",
    "        # Update and normalize\n",
    "        out1 = self.norm1(x + self.dropout(attn_output))\n",
    "        # Get update messages from feed forward\n",
    "        ff_output = self.ffn(out1) * mask\n",
    "        # Update and norm\n",
    "        out2 = self.norm2(out1 + self.dropout(ff_output))\n",
    "        return out2 * mask\n",
    "\n",
    "# Encoder combining all layers\n",
    "class EncoderTrined(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_layers, num_heads, d_model, d_ff, head_dim, input_dim, dropout,\n",
    "                    dim_middle_pred, dim_out_pred, num_middle_layers_pred):\n",
    "        super().__init__()\n",
    "        # Embedding layer\n",
    "        self.embedding_layer = nn.Linear(input_dim-1, d_model, bias=False)\n",
    "        # Encoder layers\n",
    "        self.enc_layers = nn.ModuleList([\n",
    "            EncoderLayer(num_heads, d_model, d_ff, head_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        # Prediction layer\n",
    "        self.predict_layer = PredictLayer(d_model, dim_middle_pred, dim_out_pred, num_middle_layers_pred)\n",
    "\n",
    "        # Initialize embedding layer as identity matrix \n",
    "        nn.init.eye_(self.embedding_layer.weight)\n",
    "\n",
    "    # compile NN : define loss function, optimizer, etc\n",
    "    def compile(self, optim_kwargs, scheduler_kwargs):\n",
    "        self.loss = nn.functional.mse_loss\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), **optim_kwargs)\n",
    "        self.scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.optimizer, **scheduler_kwargs)\n",
    "        metric_names = ['mse_logE_loss']\n",
    "        self.metrics = [ MeanMetrics(name=mn) for mn in metric_names ]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # Split input into data and mask\n",
    "        x, mask = inputs[:, :, :-1], inputs[:, :, -1:]\n",
    "        # Make proper attention mask\n",
    "        mask_att = (mask * torch.transpose(mask, 1, 2)).unsqueeze(1) # (bs, 1, l, l)\n",
    "        # Embed data\n",
    "        x = self.embedding_layer(x)\n",
    "        # Process through encoder layers\n",
    "        for enc_layer in self.enc_layers:\n",
    "            x = enc_layer(x, mask, mask_att)\n",
    "        # Aggregate data\n",
    "        aggr = torch.sum(x*mask, dim=1) / torch.sum(mask, dim=1)\n",
    "        # Make predictions\n",
    "        preds = self.predict_layer(aggr)\n",
    "        return preds\n",
    "\n",
    "    def update_metrics(self, metric_updates):\n",
    "        for m_update, m_tracker in zip(metric_updates, self.metrics):\n",
    "            m_tracker.update_state(m_update)\n",
    "\n",
    "    def train_step(self, data, labels):\n",
    "        # Set training regime\n",
    "        self.train()\n",
    "        # Get data\n",
    "        self.optimizer.zero_grad()\n",
    "        preds = self.forward(data)\n",
    "        loss = self.loss(preds, labels)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        # update metrics\n",
    "        self.update_metrics([loss])\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}\n",
    "\n",
    "    def test_step(self, data, labels):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = self.forward(data)\n",
    "            loss = self.loss(preds, labels)\n",
    "        # update metrics\n",
    "        self.update_metrics([loss])\n",
    "        return {**{m_tracker.name: m_tracker.result() for m_tracker in self.metrics},\n",
    "               \"learning_rate\": self.optimizer.param_groups[0]['lr']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a7d9c",
   "metadata": {},
   "source": [
    "## 7.2 Useful routines\n",
    "\n",
    "Define functions for making predictions, reweightning spectrum, and plotting 68% energy resolution (in log10 scale)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d995dc59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions\n",
    "def make_preds(model_path, steps):\n",
    "    import yaml\n",
    "    with open(model_path+'.yaml.config', 'r') as config_file:\n",
    "        config = yaml.safe_load(config_file)\n",
    "    nn = EncoderTrined(**config['nn_arch_params'])\n",
    "    nn.load_state_dict(torch.load(model_path))\n",
    "    nn.to(device)\n",
    "    # make dataset\n",
    "    _, test_dataset = make_datasets(**generator_config)\n",
    "    # predict\n",
    "    preds = []\n",
    "    for i, (data, label) in enumerate(test_dataset):\n",
    "        if i >= steps:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            pred = nn.forward(data.to(device)).to('cpu').numpy()\n",
    "        preds.append(pred)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "abf8edda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_resolutoin(preds, h5f, mode, stepE=0.1, minE=18.1, maxE=21.):\n",
    "\n",
    "    assert mode in ['differential', 'integral']\n",
    "    \n",
    "    num_take = preds.shape[0]\n",
    "    # read required fields\n",
    "    pre = 'test/'\n",
    "    with h5.File(h5f,'r') as ht:\n",
    "        m_e = ht['norm_param/recos/mean'][3]\n",
    "        s_e = ht['norm_param/recos/std'][3]\n",
    "    \n",
    "    with h5.File(h5f,'r') as hp:\n",
    "        mc_es = hp['test/mc_params'][:num_take,3]\n",
    "        particle_labels = hp['test/mc_params'][:num_take,1]\n",
    "        reco_es = hp['test/recos'][:num_take,3]\n",
    "        # back re-norm energy\n",
    "        reco_es = reco_es * s_e + m_e\n",
    "\n",
    "    # we will use only proton-induced events\n",
    "    pr_mask = particle_labels==14\n",
    "    \n",
    "    en_bins = np.arange( minE, maxE, stepE ) - 18\n",
    "    bins = np.digitize( np.log10(mc_es), en_bins )\n",
    "    \n",
    "    pr_mask = particle_labels==14\n",
    "\n",
    "    reco_ints = []\n",
    "    nn_ints = []\n",
    "    for i in range(len(en_bins)):\n",
    "        if mode=='differential':\n",
    "            ids = bins==i\n",
    "        else:\n",
    "            ids = bins>=i\n",
    "\n",
    "        part_mask = pr_mask[ids]\n",
    "\n",
    "        nn_e = preds[ids][part_mask]\n",
    "        re_e = np.log10(reco_es[ids][part_mask])\n",
    "        tr_es = np.log10(mc_es[ids][part_mask])\n",
    "        if nn_e.shape[0]!=0 and re_e.shape[0]!=0 and tr_es.shape[0]!=0:\n",
    "            # get delta\n",
    "            nn_delta = np.abs(nn_e-tr_es)/tr_es\n",
    "            re_delta = np.abs(re_e-tr_es)/tr_es\n",
    "\n",
    "            nn_ints.append(np.percentile(nn_delta, 68)) \n",
    "            reco_ints.append(np.percentile(re_delta, 68)) \n",
    "        else:\n",
    "            nn_ints.append( None )\n",
    "            reco_ints.append( None )\n",
    "            \n",
    "    fig, ax = plt.subplots( 1, 1, figsize=(6, 4) )\n",
    "    ax.set_title('68% log10 energy resolution for protons')\n",
    "\n",
    "    #reco_ints = np.array(reco_ints)\n",
    "    #nn_ints = np.array(nn_ints)\n",
    "    \n",
    "    ax.plot(18+en_bins, reco_ints, label='reconstruction')\n",
    "    ax.plot(18+en_bins, nn_ints, label='neural network')\n",
    "    \n",
    "    ax.legend()\n",
    "    ax.set(xlabel='MC Energy, log10(E/1eV)', ylabel='68% percentile')\n",
    "    \n",
    "    plt.grid()\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.3 Define path to model and plot graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m h5f \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../../taml2025_data/taml_0325_energy.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      3\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtaml_ref_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m preds \u001b[38;5;241m=\u001b[39m \u001b[43mmake_preds\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m, in \u001b[0;36mmake_preds\u001b[0;34m(model_path, steps)\u001b[0m\n\u001b[1;32m      5\u001b[0m     config \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(config_file)\n\u001b[1;32m      6\u001b[0m nn \u001b[38;5;241m=\u001b[39m EncoderTrined(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnn_arch_params\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 7\u001b[0m nn\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m nn\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# make dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:1462\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1462\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1463\u001b[0m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1464\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1465\u001b[0m \u001b[43m            \u001b[49m\u001b[43m_weights_only_unpickler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1466\u001b[0m \u001b[43m            \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1467\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1468\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1470\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(_get_wo_message(\u001b[38;5;28mstr\u001b[39m(e))) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:1964\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1962\u001b[0m \u001b[38;5;28;01mglobal\u001b[39;00m _serialization_tls\n\u001b[1;32m   1963\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m map_location\n\u001b[0;32m-> 1964\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1965\u001b[0m _serialization_tls\u001b[38;5;241m.\u001b[39mmap_location \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1967\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/_weights_only_unpickler.py:512\u001b[0m, in \u001b[0;36mUnpickler.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    505\u001b[0m         \u001b[38;5;28mtype\u001b[39m(pid) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m\n\u001b[1;32m    506\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(pid) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    507\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mserialization\u001b[38;5;241m.\u001b[39m_maybe_decode_ascii(pid[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    508\u001b[0m     ):\n\u001b[1;32m    509\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m UnpicklingError(\n\u001b[1;32m    510\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOnly persistent_load of storage is allowed, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpid[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    511\u001b[0m         )\n\u001b[0;32m--> 512\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpersistent_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpid\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m [BINGET[\u001b[38;5;241m0\u001b[39m], LONG_BINGET[\u001b[38;5;241m0\u001b[39m]]:\n\u001b[1;32m    514\u001b[0m     idx \u001b[38;5;241m=\u001b[39m (read(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m key[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m BINGET[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m unpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<I\u001b[39m\u001b[38;5;124m\"\u001b[39m, read(\u001b[38;5;241m4\u001b[39m)))[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:1928\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1926\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1927\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1928\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1929\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1930\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1932\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:1900\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1895\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1897\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1898\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1899\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1900\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1901\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1902\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1903\u001b[0m )\n\u001b[1;32m   1905\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1906\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:693\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    674\u001b[0m \u001b[38;5;124;03mRestores `storage` using a deserializer function registered for the `location`.\u001b[39;00m\n\u001b[1;32m    675\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    690\u001b[0m \u001b[38;5;124;03m       all matching ones return `None`.\u001b[39;00m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    692\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 693\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    694\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    695\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:631\u001b[0m, in \u001b[0;36m_deserialize\u001b[0;34m(backend_name, obj, location)\u001b[0m\n\u001b[1;32m    629\u001b[0m     backend_name \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_get_privateuse1_backend_name()\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(backend_name):\n\u001b[0;32m--> 631\u001b[0m     device \u001b[38;5;241m=\u001b[39m \u001b[43m_validate_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackend_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/mbp14_def_312/lib/python3.12/site-packages/torch/serialization.py:600\u001b[0m, in \u001b[0;36m_validate_device\u001b[0;34m(location, backend_name)\u001b[0m\n\u001b[1;32m    598\u001b[0m     device_index \u001b[38;5;241m=\u001b[39m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01mif\u001b[39;00m device\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_available\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device_module\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 600\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    601\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;241m.\u001b[39mupper()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    602\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice but torch.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbackend_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.is_available() is False. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    606\u001b[0m     )\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(device_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice_count\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    608\u001b[0m     device_count \u001b[38;5;241m=\u001b[39m device_module\u001b[38;5;241m.\u001b[39mdevice_count()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "h5f = '../../../taml2025_data/taml_0325_energy.h5'\n",
    "\n",
    "model_path = 'taml_ref_model'\n",
    "# This works right now only on CUDA\n",
    "preds = make_preds(model_path, steps=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_resolutoin(preds[:,0], h5f, mode='integral')\n",
    "plot_resolutoin(preds[:,0], h5f, mode='differential', stepE=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mbp14_def_312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
